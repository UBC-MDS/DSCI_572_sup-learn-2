{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lab 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import minimize, check_grad\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Exercise 0: gradients of mathematical functions\n",
    "rubric={raw:7}\n",
    "\n",
    "Compute the gradient of each of the following mathematical functions. The notation for the gradient of a function $f$ is $\\nabla f(x)$. Note that $x$ may be a vector but $f$ returns a scalar in all the cases below. The gradient is also a vector.\n",
    "\n",
    "In some cases, the dimensionality of $x$ is provided: for example\n",
    "\n",
    "> $x \\in \\mathbb{R}^3$\n",
    "\n",
    "means that $x$ is a vector with 3 elements. In other cases, you should be able to infer the dimension from the context (for example, for $f_2$ we can infer that $x \\in \\mathbb{R}^2$ since otherwise the matrix multiplication wouldn't make sense). Finally, in some cases (like $f_6$) the dimension is unknown but you should be able to give an answer that holds regardless of the dimension of $x$. \n",
    "\n",
    "Hint: for $\\nabla f_5(x)$ you can write $x^\\top A x$ as a sum of a few terms. Try taking the derivative, and then putting it back into vector notation at the end.\n",
    "\n",
    "1. $f_1(x) = \\sin(x)$ where $x\\in \\mathbb{R}$\n",
    "2. $f_2(x) = [0\\;\\; 1]x$\n",
    "3. $f_3(x) = \\exp(x_1 + x_2x_3)$ where $x \\in \\mathbb{R}^3$\n",
    "4. $f_4(x) = \\exp(x_1 + x_2x_3)$ where $x \\in \\mathbb{R}^4$\n",
    "5. $f_5(x) = x^\\top A x$ where $A=\\left[ \\begin{array}{cc}1 & 2 \\\\0 & -3 \\end{array} \\right]$\n",
    "6. $f_6(x) = x^\\top x$\n",
    "7. $f_7(x) = x_1^2\\sin(x_2)$ where $x \\in \\mathbb{R}^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: gradients of Python functions\n",
    "\n",
    "Write a Python function that computes the gradient of each of the following Python functions. \n",
    "\n",
    "Notes: \n",
    "\n",
    "- All of the functions we deal with here return a scalar, regardless of the size of the input vector `x`. We will not consider the case where the output itself is a vector, since it is not relevant for our context of loss function minimization. \n",
    "- You do not need to consider the case where `x.ndim` is 2 (or higher); assume `x.ndim` is always 1.\n",
    "- You do not need to write docstrings for your functions.\n",
    "- You can use [scipy gradient checker](http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.check_grad.html) to check your results for a few values of the inputs, as shown in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "def example(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "def example_grad(x):\n",
    "    return 2.0*x\n",
    "\n",
    "def check_grad_and_print(fun, fun_grad, dims=5):\n",
    "    x0 = np.random.rand(dims)\n",
    "    fg = fun_grad(x0)\n",
    "    if not isinstance(fg, np.ndarray):\n",
    "        print(\"Gradient should be a numpy array\")\n",
    "        return\n",
    "    if len(fg) != dims:\n",
    "        print(\"Gradient is the wrong size\")\n",
    "        return\n",
    "    \n",
    "    diff = check_grad(fun, fun_grad, np.array(x0))\n",
    "    if diff < 1e-5:\n",
    "        print('Success (probably)')\n",
    "    else:\n",
    "        print('Gradient incorrect (probably)')\n",
    "        \n",
    "check_grad_and_print(example, example_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(a)\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "def foo_grad(x):\n",
    "    \n",
    "\n",
    "check_grad_and_print(foo, foo_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(b)\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pin(x):\n",
    "    return np.sin(x[1])\n",
    "\n",
    "def pin_grad(x):\n",
    "    \n",
    "\n",
    "check_grad_and_print(pin, pin_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1(c)\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(100)\n",
    "\n",
    "def zap(x):\n",
    "    return a@x # this is matrix multiplication; equivalent to %*% in R.\n",
    "\n",
    "def zap_grad(x):\n",
    "    \n",
    "\n",
    "check_grad_and_print(zap, zap_grad, dims=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1(d)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baz(x):\n",
    "    result = 0\n",
    "    for i in range(len(x)):\n",
    "        result = result + x[i]**i\n",
    "    return result\n",
    "\n",
    "def baz_grad(x):\n",
    "    \n",
    "\n",
    "check_grad_and_print(baz, baz_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1(e)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(x):\n",
    "    if np.abs(x[1]) > 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return -(x[0]*x[0]+1)*np.cos(x[1]-1)\n",
    "\n",
    "def bar_grad(x):\n",
    "    \n",
    "    \n",
    "check_grad_and_print(bar, bar_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: gradient descent\n",
    "rubric={accuracy:10}\n",
    "\n",
    "Complete the `gradient_descent` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_grad, x0, α=0.001, ϵ=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "    Minimizes the function f given the function itself, \n",
    "    its gradient, and a starting point.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    f : function\n",
    "        the objective\n",
    "    f_grad : function\n",
    "        the gradient\n",
    "    x0 : numpy.ndarray\n",
    "        the starting point\n",
    "    α : float (optional, default=0.01)\n",
    "        the learning rate\n",
    "    ϵ : float (optional, default=0.01)\n",
    "        the tolerance for termination\n",
    "    verbose : bool (optional, default=False)\n",
    "        whether or not to print out extra output\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        the minimizer of f\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> gradient_descent(lambda x: x**2, lambda x: 2*x, 1.0, ϵ=1e-6)\n",
    "    4.990720528955806e-07\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    \n",
    "    n = 0\n",
    "    while np.linalg.norm(f_grad(x)) > ϵ:\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        if verbose and n % 100 == 0:\n",
    "            print(\"----\")\n",
    "            print(\"Iter =\", n)\n",
    "            print(\"||∇f|| =\", np.linalg.norm(f_grad(x)))\n",
    "            print(\"f =\", f(x))\n",
    "        \n",
    "        n += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tests your function by comparing the results to [`scipy.optimize.minimize`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize). You should get similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gradient_descent(pin, pin_grad, np.zeros(2), ϵ=1e-6))\n",
    "print(minimize(pin, np.zeros(2), jac=pin_grad).x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: logistic regression \n",
    "\n",
    "The code below loads [the IMDB dataset](https://www.kaggle.com/utathya/imdb-review-dataset) from DSCI 571, with positive reviews encoded as $y=+1$ and negative reviews encoded as $y=-1$. You'll need the file `imdb_master.csv` in the current directory, or you can modify the path in `read_csv` below to where you have the file stored. As a reminder, **please do not commit/push this file**. I have attempted to seed your repos with a `.gitignore` file to this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('imdb_master.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df, split, sample_size=5000,\n",
    "                   labels = ('pos','neg')):\n",
    "    \"\"\"\n",
    "    Wrangles the imdb_master data set into a test or train\n",
    "    split and takes a sample size. Returns a dataframe of the\n",
    "    single column \"review\"and an array of the numerical \n",
    "    representation of it's class label.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        the objective\n",
    "    split : str\n",
    "        the split desired either train or test.\n",
    "    sample_size : int (default : 5000)\n",
    "        the sample size from the split\n",
    "    labels : tuple, optional\n",
    "        the classification labels (default : ('pos','neg'))\n",
    "    (default : \"review\" )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: pandas.core.frame.DataFrame\n",
    "        a dataframe of the imdb review column and index\n",
    "    y: numpy.ndarray\n",
    "        the class labels \n",
    "    \"\"\"\n",
    "    \n",
    "    # get only the positive and negative reviews\n",
    "    df = df[df[\"label\"].str.startswith(labels)]\n",
    "    \n",
    "    # grab either the train or the test\n",
    "    df = df[df[\"type\"] == split] \n",
    "    \n",
    "    # subset the rows\n",
    "    df_subset = df.sample(n=sample_size, random_state=0)\n",
    "    \n",
    "    # X is the reviews\n",
    "    X = df_subset[\"review\"]\n",
    "    \n",
    "    # y is the labels, as either -1 or +1\n",
    "    y = (df_subset[\"label\"].values == labels[0])*2-1\n",
    "    return X , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reviews, y_train = transform_df(imdb_df, \"train\")\n",
    "X_test_reviews,   y_test  = transform_df(imdb_df, \"test\" )\n",
    "\n",
    "countvec = CountVectorizer(max_features=200, stop_words='english', binary = True)\n",
    "\n",
    "X_train = countvec.fit_transform(X_train_reviews).toarray()\n",
    "X_test = countvec.transform(X_test_reviews).toarray()\n",
    "\n",
    "# Note: dataset is small enough that we don't need to bother with a sparse matrix, hence toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement a logistic regression classifier \"from scratch\". You'll implement the `fit` function using [`scipy.optimize.minimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html), which does something similar to your `gradient_descent` function above (but fancier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(a)\n",
    "rubric={accuracy:10,quality:10}\n",
    "\n",
    "Complete the `fit` function, using `scipy.optimize.minimize` to solve the optimization problem. \n",
    "\n",
    "Notes: \n",
    "\n",
    "- There's no \"intercept\". This is just for simplicity. It is easy to add in a way that will shortly be explained in DSCI 573.\n",
    "- I suggest initializing your weights to all zeros.\n",
    "- You should pass in the gradient to `minimize` using the `jac` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_lr(w, X, y):\n",
    "    return np.sum(np.log(1 + np.exp(-y*(X@w))))\n",
    "\n",
    "def loss_lr_grad(w, X, y):\n",
    "    return -X.T @ (y/(1+np.exp(y*(X@w))))\n",
    "\n",
    "class MyLogisticRegression:\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X@self.w)\n",
    "\n",
    "    # Fits the regression coefficients for a logistic regression model given the data X, y\n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE AND DOCSTRING HERE\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gradient of loss\n",
    "grad_checker = check_grad(lambda w: loss_lr(w, X_train, y_train), \n",
    "                              lambda w: loss_lr_grad(w, X_train, y_train), \n",
    "                              np.zeros(X_train.shape[1]))\n",
    "print(grad_checker, \"<-- should be small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(b)\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Report your training and test error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(c)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Report a confusion matrix for both train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3(d)\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Find one false positive case and one false negative case in the test set. Print out the corresponding reviews. Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3(e)\n",
    "rubric={reasoning:1}\n",
    "\n",
    "To explore this further, pick a false positive and false negative example. Then, for each one, see what words (features) are present, and then print out the weights from the model corresponding to those words. Does your false positive example indeed contain many \"positive\" words, and vice versa for your false negative example? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(f)\n",
    "rubric={accuracy:5,reasoning:10}\n",
    "\n",
    "Adapt your code to use your `gradient_descent` function instead of `scipy.optimize.minimize`. Try using $\\alpha=10^{-4}$ and $\\epsilon=1$. Do you get similar results to what you got with `scipy.optimize.minimize`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegressionGD:\n",
    "    \n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose=verbose # whether to print stuff out during gradient descent\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X@self.w)\n",
    "\n",
    "    # Fits the regression coefficients for a logistic regression model given the data X, y\n",
    "    def fit(self, X, y):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: scikit-learn logistic regression\n",
    "rubric={reasoning:15}\n",
    "\n",
    "The scikit-learn implementation of logistic regression, which we looked at in DSCI 571, can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Compare your implementation to the sklearn one, both in terms of speed and accuracy, on the same problem as above. For a fair comparison, use the following hyperparameters with scikit-learn's `LogisticRegression`:\n",
    "\n",
    "- `C=1e8` to (mostly) disable regularization for sklearn, since your implementation doesn't use regularization. \n",
    "- `fit_intercept=False` since your code above doesn't fit an intercept term.\n",
    "- `solver=\"lbfgs\"` just in case your scikit-learn version is older than 0.22 (`minimize` is probably using [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) also.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
