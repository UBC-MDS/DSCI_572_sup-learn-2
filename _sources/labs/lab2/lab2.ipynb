{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Lab 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.optimize as spo\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.preprocessing import scale, StandardScaler, minmax_scale, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "rubric={mechanics:20}\n",
    "\n",
    "Follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: floating point errors\n",
    "\n",
    "For each of the code snippets below, explain the result. The first three are answered for you, so you can get a sense of what types of explanations we're looking for. Note that in both the second and third case we get the \"expected\" result (zero), but the explanations are very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7755575615628914e-17"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3 - 0.2 - 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample solution**: The result is not zero because 0.3, 0.2, and 0.1 are not represented exactly as floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 - 0.25 - 0.125 - 0.125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample solution**: The result is zero because 0.5, 0.25, and 0.125 are powers of 2 and there they _are_ represented exactly as floating point numbers. There is no roundoff error present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4 - 0.2 - 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample solution**: The result is correct (zero) despite the fact that 0.4 and 0.2 are not represented exactly as floating point numbers. This is a case of good luck: while 0.4 and 0.2 are not represented exactly, the roundoff errors happened to cancel out during the subtractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (a)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30 - 20 - 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (b)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30.0 - 20.0 - 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (c)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10.0**100 + 1) == 10.0**100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (d)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10.0**100000 + 1) - 10.0**100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (e)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1000) - np.exp(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (f)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/np.exp(1000) == np.exp(-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (g)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/np.exp(100) == np.exp(-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (h)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(1000)==np.exp(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (i)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.zeros(10)+0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### snippet (j)\n",
    "rubric={reasoning:5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) snippet (k)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(100000)\n",
    "x[0] = 1e20\n",
    "\n",
    "y = np.ones(100000)\n",
    "y[-1] = 1e20\n",
    "\n",
    "sum(x) == sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) snippet (l)\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(10)+0.1\n",
    "sum(x) == np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the above: see [Pairwise summation](https://en.wikipedia.org/wiki/Pairwise_summation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Exercise 2: floating point max/min\n",
    "rubric={reasoning:2}\n",
    "\n",
    "As discussed in lecture, IEEE double precision floating point numbers use 53 bits for the mantissa (one of which is a sign bit) and 11 bits for the exponent (again, one of which is a sign bit). Given thus, calculate the largest (furthest from zero) and smallest (closest to zero) possible representable floating point numbers. Then empirically check your results with Python. Are they what you expected? Discuss.\n",
    "\n",
    "NOTE: Python has a special behaviour that we need to watch out for. If you do something like `10**1000` you will get a giant integer. That's because Python has a dynamically expanding integer type. This has nothing to do with floating point representations, which are the thing we really care about for scientific computation (not to mention that most other languages, including R, do not do this). So, when playing around, make sure you write `10**1000.0` or `10.0*1000` to ensure it's a floating point. Or `1e1000`... but that doesn't work for other bases besides 10. \n",
    "\n",
    "(Also, and this is _REALLY_ out of scope but just FYI if anyone cares, in some languages `1eX` and `10^X` will return slightly different answers, if the language uses a special routine for `1eX` that is more optimized than the generral power function. I cannot imagine this ever mattering to any of us.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: logpdf\n",
    "rubric={reasoning:10}\n",
    "\n",
    "In most scientific computing environments, such as R or NumPy/SciPy, there are functions to compute the probability density function (pdf) of a probability distribution, and separate functions to compute the log of the pdf. For example, looking at the [R normal distribution documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html), you will see that you can set `log=TRUE` or `log=FALSE`. Likewise in Python we have `scipy.stats.norm.pdf` and `scipy.stats.norm.logpdf`. And yet, why bother creating these extra functions, when we can just take the log ourselves?! For example, we have NumPy functions for $\\sin(x)$ and $\\cos(x)$ but not for $\\log\\sin(x)$ and $\\log \\cos (x)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4189385332046727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.norm.logpdf(1) # is function this useless?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4189385332046727"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(scipy.stats.norm.pdf(1)) # seems to do the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3(b)\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Write a numerically safe function `log_gaussian_pdf` that computes the log of the standard (zero mean, unit variance) Gaussian PDF. Your function should produce the same result as `scipy.stats.norm.logpdf` when evaluated at $x=100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Exercise 4: $\\log(1+\\exp(z))$, continued\n",
    "rubric={accuracy:1,reasoning:1}\n",
    "\n",
    "In lecture we discussed computing $\\log(1+\\exp(z))$. We wrote a better version of the function for when $z\\gg1$, reproduced below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1_plus_exp(z):\n",
    "    return np.log(1+np.exp(z))\n",
    "\n",
    "def log_1_plus_exp_safe(z):\n",
    "    if z > 100:\n",
    "        return z\n",
    "    else:\n",
    "        return np.log(1+np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the case of $z\\ll -1$, i.e. when $z$ is a large negative number. In that case, we get zero with both of the above implementations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(log_1_plus_exp(-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(log_1_plus_exp_safe(-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tasks:\n",
    "\n",
    "1. Investigate why this is happening. Is the problem that $\\exp(-100)$ itself underflows?\n",
    "2. Write a function `log_1_plus_exp_safer` that works well when $z\\gg 1$ and $z\\ll -1$.\n",
    "3. For what range of values of $z$ does your `log_1_plus_exp_safer` function give reasonable results?\n",
    "4. Your code presumably contains two thresholds, the upper and lower cutoffs for $z$ at which the approximations are invoked. Can you reason about the \"optimal\" or \"best\" values for these thresholds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: softmax logistic regression and log-sum-exp\n",
    "rubric={reasoning:15,accuracy:5}\n",
    "\n",
    "In the \"multinomial\" (aka softmax) approach to multi-class logistic regression, your loss ends up having one term for each class, so you get something of the form $\\log \\sum_{k=1}^c \\exp(z_k)$, where $c$ is the number of classes, and $z_k$ is a quantity involving the weights and training data. For any choice of a constant $a$, we can rewrite this as follows:\n",
    "\n",
    "$$\\begin{align}\\log \\displaystyle\\sum_{k=1}^c \\exp(z_k) &= \\log \\displaystyle\\sum_{k=1}^c \\exp(z_k-a+a)\\\\ &= \\log \\left( \\displaystyle\\sum_{k=1}^c \\exp(z_k-a)\\exp(a) \\right) \\\\ &= \\log \\left( \\exp(a)\\displaystyle\\sum_{k=1}^c \\exp(z_k-a)\\right) \\\\ &= \\log \\left( \\exp(a) \\right) + \\log\\displaystyle\\sum_{k=1}^c \\exp(z_k-a) \\\\ &= a+ \\log \\displaystyle\\sum_{k=1}^c \\exp(z_k-a)\\end{align}$$\n",
    "\n",
    "Note: you only need to look at the first and last expression - the middle parts are only there to convince you they are indeed equivalent.\n",
    "\n",
    "1. Explain why this final expression might be more numerically stable and why $a=\\max \\{z_1,z_2,\\ldots,z_c\\}$ is a sensible choice.\n",
    "2. If $a=\\max \\{z_1,z_2,\\ldots,z_c\\}$, this trick seems to rely on the fact that overflow is more of a danger than underflow, because we may now compute $exp$ of some very large negative values if $a$ is large. Explain why overflow is more of a problem than underflow in this calculation.\n",
    "3. Write a python function `log_sum_exp` that takes in an array `z` and computes the _original_ expression. Write another function `log_sum_exp_stable` that computes the final (more stable) expression using $a=\\max \\{z_1,z_2,\\ldots,z_c\\}$. Discuss the behaviour of the two functions. Also, compare your implementation to In fact, [`scipy.special.logsumexp`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.logsumexp.html#scipy.special.logsumexp) for one or two cases.\n",
    "4. (optional) Earlier, we used the approximation $\\log(1+\\exp(z))\\approx z$ when $z\\gg 1$. Is that just a specific case of what we have here? It seems that what we did earlier was an approximation, whereas what we did here is mathematically exact. Do you think there is any significance to this distinction, in practice?\n",
    "\n",
    "(FYI: you'll see this trick in real implementations of ML algorithms! )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: SGD implementation\n",
    "\n",
    "Below is a Python function that performs gradient descent, much like the code you wrote in lab 1. I just made a few small changes:\n",
    "\n",
    "- Removed the docstring and `print` statements for brevity.\n",
    "- Renamed `x` to `w`.\n",
    "- In this version, the user specifies a fixed number of iterations `n_iters` instead of a tolerance $\\epsilon$.\n",
    "- In this version, the user passes in `X` and `y` explicitly, and these are passed to `f_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_grad, w0, X, y, n_iters=1000, α=0.001):\n",
    "    w = w0\n",
    "    for t in range(n_iters):\n",
    "        w = w - α*f_grad(w, X, y)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code works (although is very slow as per usual for gradient descent). For example, here we do ordinary least squares linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "X = minmax_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_ols(w,X,y): return 0.5*np.sum((X@w-y)**2)\n",
    "def loss_ols_grad(w,X,y): return X.T@(X@w) - X.T@y\n",
    "\n",
    "w0 = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gd = gradient_descent(loss_ols, loss_ols_grad, w0, X, y, n_iters=10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.37446749,   4.79336123,   3.71903592,   2.67885881,\n",
       "        -2.90900155,  36.50466055,   2.10874674,  -4.55318591,\n",
       "         5.45719841,  -6.02418944,  -4.39530169,   9.88901723,\n",
       "       -11.79090734])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=False).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.37446749,   4.79336123,   3.71903592,   2.67885881,\n",
       "        -2.90900155,  36.50466055,   2.10874674,  -4.55318591,\n",
       "         5.45719841,  -6.02418944,  -4.39530169,   9.88901723,\n",
       "       -11.79090734])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the coefficients obtained from gradient descent are very similar to those obtained by scikit-learn. All is well so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6(a)\n",
    "rubric={accuracy:20,quality:10}\n",
    "\n",
    "Your task is to implement a function `stochastic_gradient_descent`, that performs SGD, _using_ the `gradient_descent` function provided above. You can have your function accept the same arguments as the `gradient_descent` function above, except:\n",
    "\n",
    "- Change `n_iters` to `n_epochs`.\n",
    "- Add an extra `batch_size` argument.\n",
    "\n",
    "Note: even though this might not be a good idea in general, you can leave $\\alpha$ constant in this implementation; that is, you can use the same $\\alpha$ across all iterations. \n",
    "\n",
    "Note: the pedagogical goal here is to help you see how SGD relates to regular gradient descent. In reality it would be fine to implement SGD \"from scratch\" without calling a GD function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6(b)\n",
    "rubric={reasoning:5}\n",
    "\n",
    "Show that when the batch size is set to the whole training set, you get exactly the same results as with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: `SGDClassifier` and `SGDRegresor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we'll explore training a classifier with SGD on the [Sentiment140 dataset](http://help.sentiment140.com/home), which contains tweets labeled with sentiment associated with a brand, product, or topic. Please start by doing the following:\n",
    "\n",
    "1. Download the corpus from [here](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip).\n",
    "2. Unzip.\n",
    "3. Copy the file `training.1600000.processed.noemoticon.csv` into the current directory.\n",
    "4. Create a `.gitignore` file so that you don't accidentally commit the dataset.\n",
    "\n",
    "Once you're done the above, steps, run the starter code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "tweets_df = pd.read_csv('training.1600000.processed.noemoticon.csv', \n",
    "                        encoding = \"ISO-8859-1\",\n",
    "                        names=[\"label\",\"id\", \"date\", \"no_query\", \"name\", \"text\"])\n",
    "tweets_df['label'] = tweets_df['label'].map({0: 'neg', 4: 'pos'})\n",
    "\n",
    "tweets_df.head()\n",
    "\n",
    "# only consider positive and negative reviews\n",
    "tweets_pos_neg_df = tweets_df[tweets_df['label'].str.startswith(('pos','neg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pos_neg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df_train, tweets_df_test = train_test_split(tweets_pos_neg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we encode it using `CountVectorizer`, which may take a minute or so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train = vec.fit_transform(tweets_df_train['text']) \n",
    "y_train = tweets_df_train['label']\n",
    "\n",
    "# just use the already-fit vectorizer\n",
    "X_test = vec.transform(tweets_df_test['text']) \n",
    "y_test = tweets_df_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our training data is rather large compared to datasets we've explored in the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the fraction of elements that are nonzero. Having a sparse matrix really helps!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.nnz/np.prod(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a classifier. I'll use `time` instead of `%timeit` because I want to keep the output, and it gets lost with `%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training took %.1f seconds\" % (time.time() - t));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7(a)\n",
    "rubric={reasoning:10}\n",
    "\n",
    "Train a logistic regression model on the same dataset using [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) instead of `LogisticRegression`. Compare the training time.\n",
    "\n",
    "FYI: there's also a regression version, [`SGDRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html), which does linear regression with SGD. However, we won't explore `SGDRegressor` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7(b)\n",
    "rubric={reasoning:15}\n",
    "\n",
    "Discuss the training and test accuracies of the two approaches. Can you explain what you're seeing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) 7(c)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "One possible explanation for `SGDClassifier`'s speed is that it's just doing fewer iterations/epochs. `SGDClassifier` and `LogisticRegression` have an `n_iter_` attribute which you can check after fitting. Compare these values and discuss them in the context of the above hypothesis. Then, using the `max_iter` parameter, do a \"fair\" experiment where `SGDClassifier` and `LogisticRegression` do the same number of passes through the dataset, and comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
