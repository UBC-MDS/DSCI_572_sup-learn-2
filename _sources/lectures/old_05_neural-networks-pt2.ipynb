{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dsci572_header.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Training Neural Networks\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Learning Objectives\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain how backpropagation works at a high level\n",
    "\n",
    "- Describe the difference between training loss and validation loss when creating a neural network\n",
    "- Identify and describe common techniques to avoid overfitting/apply regularization to neural networks, e.g., early stopping, drop out, L2 regularization\n",
    "- Use `PyTorch` to develop a fully-connected neural network and training pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms, datasets, utils\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils.plotting import *\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Differentiation, Backpropagation, Autograd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- In previous lectures we've discussed optimization algorithms like gradient descent, stochastic gradient descent, ADAM, etc.\n",
    "\n",
    "- These algorithms need the gradient of the loss function w.r.t the model parameters to optimize the parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- We've been able to calculate the gradient analytically for linear and logistic regression\n",
    "\n",
    "- But how would you work out the gradient for this *very* simple network for regression:\n",
    "\n",
    "![](img/backprop-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The equation for calculating the output of that network is below, it's the linear layers and activation functions (Sigmoid in this case) recursively stuck together:\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$\\hat{y}=w_3\\sigma(w_1x+b_1) + w_4\\sigma(w_2x+b_2) + b_3$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- So how would we calculate the gradient of say the MSE loss w.r.t to all our parameters?\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum^{n}_{i=1}(y_i-\\hat{y_i})^2$$ \n",
    "\n",
    "$$\\nabla \\mathcal{L}(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial L}{\\partial w_1} \\\\ \\frac{\\partial L}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial w_d} \\end{bmatrix}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We have 3 options:\n",
    "\n",
    "1. **Symbolic differentiation**: analytical derivation or \"do it by hand\", like we learned in calculus.\n",
    "\n",
    "2. **Numerical differentiation**: using finite difference methods $\\frac{df}{dx} \\approx \\frac{f(x+h)-f(x)}{h}$.\n",
    "\n",
    "3. **Automatic differentiation**: breaking down computations into elementary sequential steps (the derivatives of which we know analytically), and using the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be looking at Automatic Differentiation (AD) to train neural networks.\n",
    "\n",
    "If you're interested in learning more about the other methods, see [Appendix C](appendixC_computing-derivatives.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's go through a short example based on this network:\n",
    "\n",
    "![](img/backprop-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Let's decompose that into smaller operations. Let's introduce some new variables to hold intermediate states $z_i$ (node output before activation) and $a_i$ (node output after activation)\n",
    "\n",
    "- We'll feed in one sample data point `(x, y)` = `(1, 3)`.\n",
    "- Let's carry out forward pass. \n",
    "- We're showing intermediate outputs in green and the final loss in red. \n",
    "\n",
    "![](img/backprop-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now let's zoom in to the output node and calculate the gradients for just the parameters connected to that node\n",
    "\n",
    "- It looks complicated but the derivatives are very simple; take some time to examine this figure.\n",
    "\n",
    "![](img/backprop-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine the impact of the weight $w_3$ on the loss $\\mathcal{L}$. Note that $w_3$ is not explicitly part of the loss function $\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y_i})^2$. \n",
    "\n",
    "However, $w_3$ influences the loss indirectly through the calculation of $\\hat{y}$. A change in $w_3$ influences the prediction $\\hat{y}$, which is then used in the calculation of the loss $\\mathcal{L}$. \n",
    "\n",
    "To compute gradient of $\\mathcal{L}$ with respect to $w_3$, we need to track these cascade of effects using the chain rule. Here is how we apply it in this context: \n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{L}}}{\\partial{w_3}} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{w_3}}$$\n",
    "\n",
    "- From $w_3$ to $\\hat{y}$: $\\frac{\\partial{\\hat{y}}}{\\partial{w_3}}$ = $\\frac{\\partial{(w_3a_1 + w_4a_2 + b_3)}}{\\partial{w_3}} = a_1$ because $w_4$, $a_1$, $a_2$, and $b_3$ are constants when we calculate derivative with respect to $w_3$. \n",
    "- From $\\hat{y}$ to $\\mathcal{L}$: The $\\frac{\\partial{L}}{\\partial{\\hat{y}}}$ in this context is the derivative of MSE loss function and we have calculated it as -3.31. So, \n",
    "$$\\frac{\\partial{\\mathcal{L}}}{\\partial{w_3}} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{w_3}} = -3.31 \\times a_1$$\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$$\\frac{\\partial{\\mathcal{L}}}{\\partial{w_4}} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{w_4}} = -3.31 \\times a_2$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- That all boils down to this:\n",
    "\n",
    "![](img/backprop-5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now, the beauty of backpropagation is that we can use these results to easily calculate derivatives earlier in the network using the *chain rule*\n",
    "\n",
    "- I'll do that for $b_1$ and $b_2$ below\n",
    "\n",
    "- Once again, it looks complicated, but we're simply combining a bunch of small, simple derivatives with the chain rule.\n",
    "\n",
    "![](img/backprop-6.png)\n",
    "\n",
    "**Note:** $ \\frac{d}{dx} \\sigma (x) =  \\sigma (x) (1 - \\sigma (x))$, where $\\sigma (x) = \\text{Sigmoid}(x)$ in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to determine the impact of the bias $b_1$ on the loss $\\mathcal{L}$. Note that $b_1$ is not explicitly part of the loss function $\\mathcal{L}$. It influences the loss indirectly through a series of calculations within the network. \n",
    "- A change in $b_1$ alters the weighted input $z_1 = w_1x + b_1$. \n",
    "- This change in $z_1$ modifies the activation $a_1$, which is the output of the sigmoid function applied to $z_1$.\n",
    "- The activation $a_1$ then contributes to the subsequent layer's computations, ultimately influencing the predicted output $\\hat{y}$.\n",
    "- Finally, $\\hat{y}$ is used in the calculation of the loss $\\mathcal{L}$. \n",
    "\n",
    "To compute the gradient of $\\mathcal{L}$ with respect to $b_1$, we need to track this cascade of effects using the chain rule, which will allow us to connect the rate of change of each variable in the pathway from $b_1$ to $\\mathcal{L}$.  \n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b_1}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{a_1}}\\frac{\\partial{a_1}}{\\partial{z_1}}\\frac{\\partial{z_1}}{\\partial{b_1}}$$\n",
    "\n",
    "- From $b_1$ to $z_1$: $z_1$ is a linear combination $w_1x + b_1$. Since $w_1$ and $x$ are constant when calculating the derivative wrt $b_1$, $\\frac{\\partial{z_1}}{\\partial{b_1}} = 1$  \n",
    "- From $z_1$ to $a_1$: The derivative of a sigmoid function $\\sigma(z_1)$ with respect to $z_1$ is $\\sigma(z_1).\\sigma(1-\\sigma{z_1})$. Since $a_1 = \\sigma{z_1}$, this simplifies to $a_1.(1 - a_1) = 0.88 \\times (1 - 0.88) = 0.1056$\n",
    "- From $a_1$ to $\\hat{y}$: This part requires understanding how $a_1$ is used to calculate $\\hat{y}$. $\\frac{\\partial{\\hat{y}}}{\\partial{a_1}} = \\frac{\\partial{(w_3a_1 + w_4a_2 + b_3)}}{\\partial{a_1}} = w_3 = 1$\n",
    "- From $\\hat{y}$ to $\\mathcal{L}$: The $\\frac{\\partial{L}}{\\partial{\\hat{y}}}$ in this context is the derivative of MSE loss function and we have calculated it as -3.31. So,\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b_1}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}}\\frac{\\partial{\\hat{y}}}{\\partial{a_1}}\\frac{\\partial{a_1}}{\\partial{z_1}}\\frac{\\partial{z_1}}{\\partial{b_1}} = -3.31 \\times 1 \\times 0.11 \\times 1 = -0.35$$\n",
    "\n",
    "This gradient tells us how small changes in $b_1$ will affect the overall loss $\\mathcal{L}$, guiding us in updating the bias $b_1$, during the training process to minimize $\\mathcal{L}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1056"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((0.88)*(1 - 0.88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.349536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3.31 * 1 * ((0.88)*(1 - 0.88)) * 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- I've left calculating the gradients of $w_1$ and $w_2$ up to you\n",
    "\n",
    "- All the gradients for the network boil down to this:\n",
    "\n",
    "![](img/backprop-7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- So summarizing the process:\n",
    "\n",
    "    1. Do one \"forward pass\" of the data through the network to compute the loss value\n",
    "\n",
    "    2. Do one \"backward pass\" through the network to calculate gradients of the loss w.r.t parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The **backward pass** above is actually a special case of automatic differentiation called **reverse-mode automatic differentiation**\n",
    "\n",
    "- In the context of neural networks, the process of computing gradients of the loss w.r.t. model parameters using reverse-mode automatic differentiation is known as the **backpropagation algorithm**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autograd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- `torch.autograd` is PyTorch's automatic differentiation engine which helps us implement backpropagation\n",
    "\n",
    "- In simple terms, `torch.autograd` automatically calculates and stores derivatives for your network\n",
    "\n",
    "- Consider our simple network above:\n",
    "\n",
    "![](img/backprop-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a PyTorch class for our toy network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class network(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.4274],\n",
       "                      [ 0.2043]])),\n",
       "             ('hidden.bias', tensor([ 0.3678, -0.5838])),\n",
       "             ('output.weight', tensor([[-0.6614, -0.4467]])),\n",
       "             ('output.bias', tensor([0.3320]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = network(1, 2, 1)  # make an instance of our network\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current weights are randomly initialized. Let's initialize them according to our toy network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.state_dict()['hidden.weight'][:] = torch.tensor([[1], [-1]])  # fix the weights manually based on the earlier figure\n",
    "model.state_dict()['hidden.bias'][:] = torch.tensor([1, 2])\n",
    "model.state_dict()['output.weight'][:] = torch.tensor([[1, 2]])\n",
    "model.state_dict()['output.bias'][:] = torch.tensor([-1])\n",
    "x, y = torch.tensor([1.0]), torch.tensor([3.0])  # our x, y data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now let's check the gradient of the bias of the output node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.output.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- It's currently `None`\n",
    "\n",
    "- PyTorch is tracking the operations in our network and how to calculate the gradient, but it hasn't calculated anything yet because we don't have a loss function and we haven't done a forward pass to calculate the loss so there's nothing to backpropagate yet\n",
    "- Let's define a loss now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now we can force Pytorch to \"backpropagate\" the errors, like we just did by hand earlier by:\n",
    "\n",
    "    1. Doing a \"forward pass\" of our `(x, y)` data and calculating the `loss`;\n",
    "    \n",
    "    2. \"Backpropagating\" the loss by calling `loss.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = criterion(model(x), y)\n",
    "loss.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now let's check the gradient of the bias of the output node ($\\frac{\\partial L}{\\partial b_3}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.3142])\n"
     ]
    }
   ],
   "source": [
    "print(model.output.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- It matches what we calculated earlier!\n",
    "\n",
    "![](img/backprop-8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- We can make sure that all our gradients match what we calculated by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Gradients\n",
      "Bias: tensor([-0.3480, -1.3032])\n",
      "Weights: tensor([-0.3480, -1.3032])\n",
      "\n",
      "Output Layer Gradients\n",
      "Bias: tensor([-3.3142])\n",
      "Weights: tensor([-2.9191, -2.4229])\n"
     ]
    }
   ],
   "source": [
    "print(\"Hidden Layer Gradients\")\n",
    "print(\"Bias:\", model.hidden.bias.grad)\n",
    "print(\"Weights:\", model.hidden.weight.grad.squeeze())\n",
    "print()\n",
    "print(\"Output Layer Gradients\")\n",
    "print(\"Bias:\", model.output.bias.grad)\n",
    "print(\"Weights:\", model.output.weight.grad.squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now that we have the gradients, what's the next step? We use our optimization algorithm to update our weights\n",
    "\n",
    "- These are our current weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 1.],\n",
       "                      [-1.]])),\n",
       "             ('hidden.bias', tensor([1., 2.])),\n",
       "             ('output.weight', tensor([[1., 2.]])),\n",
       "             ('output.bias', tensor([-1.]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- To optimize them, we:\n",
    "\n",
    "    1. Define an `optimizer`,\n",
    "    \n",
    "    2. Ask it to update our weights based on our gradients using `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Our weights should now be different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 1.0348],\n",
       "                      [-0.8697]])),\n",
       "             ('hidden.bias', tensor([1.0348, 2.1303])),\n",
       "             ('output.weight', tensor([[1.2919, 2.2423]])),\n",
       "             ('output.bias', tensor([-0.6686]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- One last thing for you to know: **PyTorch does not automatically clear the gradients** after using them\n",
    "\n",
    "- So if I call `loss.backward()` again, my gradients accumulate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3 gradient after call 1 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 2 of loss.backward(): tensor([-0.3983, -1.1953])\n",
      "b3 gradient after call 3 of loss.backward(): tensor([-0.5974, -1.7929])\n",
      "b3 gradient after call 4 of loss.backward(): tensor([-0.7966, -2.3906])\n",
      "b3 gradient after call 5 of loss.backward(): tensor([-0.9957, -2.9882])\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()  # <- I'll explain this in the next cell\n",
    "\n",
    "for _ in range(1, 6):\n",
    "\n",
    "    loss = criterion(model(x), y)\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"b3 gradient after call {_} of loss.backward():\", model.hidden.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Our gradients are accumulating each time we call `loss.backward()`\n",
    "\n",
    "- So we need to tell PyTorch to \"zero the gradients\" in each iteration using `optimizer.zero_grad()`:\n",
    "\n",
    "- If you don't reset the gradients after each update, the gradients computed by subsequent backward passes will add to the existing gradients rather than replacing them. This leads to incorrect gradient values, as each pass's gradients will be influenced by the previous passes, rather than solely reflecting the current pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3 gradient after call 1 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 2 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 3 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 4 of loss.backward(): tensor([-0.1991, -0.5976])\n",
      "b3 gradient after call 5 of loss.backward(): tensor([-0.1991, -0.5976])\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1, 6):\n",
    "\n",
    "    optimizer.zero_grad()  # <- don't forget this\n",
    "    \n",
    "    loss = criterion(model(x), y)\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"b3 gradient after call {_} of loss.backward():\", model.hidden.bias.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note**: You might wonder why PyTorch behaves like this. There are two notable reasons for this:\n",
    "\n",
    "1. It is a common situation that we want to use a certain batch size but we can't, due to hardware limitations. In order to imitate a large batch size, we can use smaller batches and compute gradients in each iteration, and instead of updating the weights, we can simply accumulate the gradients for a certain number of iterations. Since the gradient is the sum of contributions from each data point, we can wait for as many iterations as we want, and only then update the weights using the accumulated gradient. In this way, we have effectively used a large batch size without causing potential memory problems.\n",
    "\n",
    "2. When our overall loss is the sum of multiple losses, we would want to accumulate gradients when we do `.backward()` on each loss so as to be able to arrive at the gradient of the overall loss in the end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Optional) Computational Graph "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "PyTorch's `autograd` basically keeps a record of our data and network operations in a computational graph. That's beyond the scope of this lecture, but if you're interested in learning more, I recommend this [blog post](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/) from Pytorch's website.\n",
    "\n",
    "Also, `torchviz` is a useful package to look at the \"computational graph\" PyTorch is building for us under the hood. For using `torchviz`, you should first install `graphviz` using this [link](https://graphviz.org/download/), and then install `torchviz` using pip:\n",
    "```bash\n",
    "pip install torchviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"348pt\" height=\"416pt\"\n",
       " viewBox=\"0.00 0.00 348.00 415.75\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 411.75)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-411.75 344,-411.75 344,4 -4,4\"/>\n",
       "<!-- 6608206832 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>6608206832</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"203,-31.25 145,-31.25 145,0 203,0 203,-31.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-5.75\" font-family=\"monospace\" font-size=\"10.00\"> (1, 1)</text>\n",
       "</g>\n",
       "<!-- 6608420928 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6608420928</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"224,-86.5 124,-86.5 124,-67.25 224,-67.25 224,-86.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 6608420928&#45;&gt;6608206832 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>6608420928&#45;&gt;6608206832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174,-66.88C174,-60.39 174,-51.29 174,-42.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-42.71 174,-32.71 170.5,-42.71 177.5,-42.71\"/>\n",
       "</g>\n",
       "<!-- 6608421072 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6608421072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"100,-141.75 0,-141.75 0,-122.5 100,-122.5 100,-141.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6608421072&#45;&gt;6608420928 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6608421072&#45;&gt;6608420928</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.03,-122.09C90.72,-113.64 120.38,-100.9 142.81,-91.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144.05,-94.11 151.86,-86.95 141.29,-87.68 144.05,-94.11\"/>\n",
       "</g>\n",
       "<!-- 6600933648 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>6600933648</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"91,-208.25 9,-208.25 9,-177.75 91,-177.75 91,-208.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-194.75\" font-family=\"monospace\" font-size=\"10.00\">output.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50\" y=\"-183.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 6600933648&#45;&gt;6608421072 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6600933648&#45;&gt;6608421072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50,-177.45C50,-170.12 50,-161.12 50,-153.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.5,-153.2 50,-143.2 46.5,-153.2 53.5,-153.2\"/>\n",
       "</g>\n",
       "<!-- 6608421120 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>6608421120</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"230,-141.75 118,-141.75 118,-122.5 230,-122.5 230,-141.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"174\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">SigmoidBackward0</text>\n",
       "</g>\n",
       "<!-- 6608421120&#45;&gt;6608420928 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>6608421120&#45;&gt;6608420928</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M174,-122.33C174,-115.67 174,-106.24 174,-97.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.5,-97.86 174,-87.86 170.5,-97.86 177.5,-97.86\"/>\n",
       "</g>\n",
       "<!-- 6608421216 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>6608421216</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"222,-202.62 122,-202.62 122,-183.38 222,-183.38 222,-202.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"172\" y=\"-189.12\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 6608421216&#45;&gt;6608421120 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>6608421216&#45;&gt;6608421120</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.3,-183.06C172.57,-175.1 172.98,-163.19 173.33,-152.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.85,-153.26 173.69,-143.15 169.86,-153.03 176.85,-153.26\"/>\n",
       "</g>\n",
       "<!-- 6608421360 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6608421360</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"122,-269.12 22,-269.12 22,-249.88 122,-249.88 122,-269.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"72\" y=\"-255.62\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6608421360&#45;&gt;6608421216 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6608421360&#45;&gt;6608421216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M85.9,-249.53C102.16,-239.05 129.3,-221.54 148.77,-208.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.38,-211.46 156.89,-203.1 146.59,-205.58 150.38,-211.46\"/>\n",
       "</g>\n",
       "<!-- 6600934992 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6600934992</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"112,-341.25 30,-341.25 30,-310.75 112,-310.75 112,-341.25\"/>\n",
       "<text text-anchor=\"middle\" x=\"71\" y=\"-327.75\" font-family=\"monospace\" font-size=\"10.00\">hidden.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"71\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\"> (2)</text>\n",
       "</g>\n",
       "<!-- 6600934992&#45;&gt;6608421360 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6600934992&#45;&gt;6608421360</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.23,-310.36C71.37,-301.38 71.55,-289.76 71.7,-280.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.21,-280.38 71.86,-270.32 68.21,-280.27 75.21,-280.38\"/>\n",
       "</g>\n",
       "<!-- 6608421312 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>6608421312</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-269.12 141,-269.12 141,-249.88 217,-249.88 217,-269.12\"/>\n",
       "<text text-anchor=\"middle\" x=\"179\" y=\"-255.62\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 6608421312&#45;&gt;6608421216 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6608421312&#45;&gt;6608421216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.03,-249.53C177.02,-240.29 175.43,-225.58 174.13,-213.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.52,-213.46 172.97,-203.89 170.56,-214.21 177.52,-213.46\"/>\n",
       "</g>\n",
       "<!-- 6608421408 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>6608421408</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"230,-335.62 130,-335.62 130,-316.38 230,-316.38 230,-335.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-322.12\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6608421408&#45;&gt;6608421312 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6608421408&#45;&gt;6608421312</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.86,-316.03C179.72,-306.79 179.49,-292.08 179.3,-280.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.79,-280.34 179.14,-270.39 175.79,-280.45 182.79,-280.34\"/>\n",
       "</g>\n",
       "<!-- 6600935184 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>6600935184</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"227,-407.75 133,-407.75 133,-377.25 227,-377.25 227,-407.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-394.25\" font-family=\"monospace\" font-size=\"10.00\">hidden.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"180\" y=\"-383\" font-family=\"monospace\" font-size=\"10.00\"> (2, 1)</text>\n",
       "</g>\n",
       "<!-- 6600935184&#45;&gt;6608421408 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>6600935184&#45;&gt;6608421408</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180,-376.86C180,-367.88 180,-356.26 180,-346.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"183.5,-346.82 180,-336.82 176.5,-346.82 183.5,-346.82\"/>\n",
       "</g>\n",
       "<!-- 6608421168 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>6608421168</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"325,-141.75 249,-141.75 249,-122.5 325,-122.5 325,-141.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"287\" y=\"-128.25\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 6608421168&#45;&gt;6608420928 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>6608421168&#45;&gt;6608420928</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.83,-122.09C250.06,-113.72 223.36,-101.13 202.98,-91.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.8,-88.05 194.26,-86.95 201.82,-94.38 204.8,-88.05\"/>\n",
       "</g>\n",
       "<!-- 6608421456 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>6608421456</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"340,-202.62 240,-202.62 240,-183.38 340,-183.38 340,-202.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"290\" y=\"-189.12\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 6608421456&#45;&gt;6608421168 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>6608421456&#45;&gt;6608421168</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M289.54,-183.06C289.14,-175.1 288.53,-163.19 288.01,-152.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.46,-152.96 287.46,-143.15 284.47,-153.32 291.46,-152.96\"/>\n",
       "</g>\n",
       "<!-- 6600933552 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>6600933552</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"337,-274.75 243,-274.75 243,-244.25 337,-244.25 337,-274.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"290\" y=\"-261.25\" font-family=\"monospace\" font-size=\"10.00\">output.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"290\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\"> (1, 2)</text>\n",
       "</g>\n",
       "<!-- 6600933552&#45;&gt;6608421456 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>6600933552&#45;&gt;6608421456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290,-243.86C290,-234.88 290,-223.26 290,-213.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"293.5,-213.82 290,-203.82 286.5,-213.82 293.5,-213.82\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x189728d10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchviz\n",
    "torchviz.make_dot(model(torch.ones(1, 1)), params=dict(model.named_parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The backpropagation algorithm uses the chain rule to compute the gradients\n",
    "\n",
    "- The chain rule involves multiplying potentially many terms together (depending on the depth of our neural network)\n",
    "\n",
    "- Therefore, we are potentially in danger of vanishing/exploding computations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Especially when using activation functions such as sigmoid, the situation gets worse:\n",
    "\n",
    "<img src=\"http://ronny.rest/media/blog/2017/2017_08_10_sigmoid/sigmoid_and_derivative_plot.jpg\" width=\"600\">\n",
    "\n",
    "([image source](http://ronny.rest/media/blog/2017/2017_08_10_sigmoid/sigmoid_and_derivative_plot.jpg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible solutions:\n",
    "\n",
    "- Using the ReLU activation\n",
    "\n",
    "- Batch normalization\n",
    "\n",
    "- Change learning rate\n",
    "\n",
    "- ResNets\n",
    "\n",
    "- Changing network architecture\n",
    "\n",
    "- Gradient clipping\n",
    "\n",
    "- Different weight initialization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Neural Networks\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The big takeaway from the last section is that PyTorch's `autograd` takes care of the gradients for us\n",
    "\n",
    "- We just need to put all the pieces together properly\n",
    "- Remember the below `trainer()` function I used last lecture to train my network\n",
    "- Now we know what all this means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, dataloader, epochs=5):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        losses = 0\n",
    "        \n",
    "        for X, y in dataloader:  # for each batch\n",
    "            \n",
    "            optimizer.zero_grad()            # Zero all the gradients w.r.t. parameters\n",
    "            \n",
    "            y_hat = model(X).flatten()       # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)       # Calculate loss based on output\n",
    "            loss.backward()                  # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()                 # Update parameters\n",
    "\n",
    "            losses += loss.item()            # Add loss for this batch to running total\n",
    "            \n",
    "        train_loss.append(losses / len(dataloader))  # loss = total loss in epoch / number of batches = loss per batch\n",
    "        \n",
    "    return train_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Notice how I calculate the loss for each epoch by summing up the loss for each batch in that epoch\n",
    "\n",
    "- I then divide the loss for each epoch by the total number of batches to get the average loss per batch in an epoch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If our model is being trained correctly, our loss should go down over time. Let's try it out with some sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "X = torch.arange(-3, 3, 0.15)\n",
    "y = X ** 2 + X * torch.normal(0, 1, (40,))\n",
    "\n",
    "dataloader = DataLoader(TensorDataset(X[:, None], y), batch_size=1, shuffle=True)\n",
    "\n",
    "plot_regression(X, y, y_range=[-1, 10], dy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = network(1, 3, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = trainer(model, criterion, optimizer, dataloader, epochs=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_regression(X, y, model(X[:, None]).detach(), y_range=[-1, 10], dy=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The model looks like a good fit, so presumably the loss went down as epochs progressed, let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(train_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation Loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- We've been focusing on training loss so far, but as we know, we need to validate our model on new **unseen** data\n",
    "\n",
    "- For this, we'll need some validation data. I'm going to split our dataset in half to create a `trainloader` and a `validloader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "torch.manual_seed(0)\n",
    "\n",
    "X_valid = torch.arange(-3.0, 3.0)\n",
    "y_valid = X_valid ** 2\n",
    "\n",
    "trainloader = DataLoader(TensorDataset(X, y), batch_size=1, shuffle=True)\n",
    "validloader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=1, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- The wonderful thing about PyTorch is that you are in full control, you can do whatever you want\n",
    "\n",
    "- So here, after each epoch, I'm going to record the validation loss by looping over my validation batches, it's just a little extra module I add to my training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "\n",
    "            y_hat = model(X).flatten()  # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "\n",
    "        train_loss.append(train_batch_loss / len(trainloader))\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood\n",
    "\n",
    "            for X_valid, y_valid in validloader:\n",
    "\n",
    "                y_hat = model(X_valid).flatten()  # Forward pass to get output\n",
    "                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n",
    "\n",
    "                valid_batch_loss += loss.item()\n",
    "            \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = network(1, 6, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "train_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201)\n",
    "\n",
    "plot_loss(train_loss, valid_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- What do we see above?\n",
    "\n",
    "- Well, we're obviously overfitting. We are optimizing too well\n",
    "\n",
    "- One way we could avoid overfitting is by terminating the training if our validation loss starts going up, this is called **early stopping**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Early stopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Early stopping is one way of avoiding overfitting\n",
    "\n",
    "- As training progresses, if we notice the validation loss increasing (while the training loss is decreasing), that's usually an indication of overfitting\n",
    "- The validation loss may go up and down from epoch to epoch, so usually we define a **patience** parameter which is a number of consecutive epochs we're willing to allow the validation loss to increase before we stop\n",
    "- The beauty of PyTorch is how easy and intuitive it is to customize your network in this way, unlike with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        \n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            \n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            \n",
    "            y_hat = model(X).flatten()  # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "            \n",
    "        train_loss.append(train_batch_loss / len(trainloader))\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "\n",
    "            for X_valid, y_valid in validloader:\n",
    "\n",
    "                y_hat = model(X_valid).flatten()  # Forward pass to get output\n",
    "                loss = criterion(y_hat, y_valid)  # Calculate loss based on output\n",
    "\n",
    "                valid_batch_loss += loss.item()\n",
    "            \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 0 and valid_loss[-1] > valid_loss[-2]:\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "        if consec_increases == patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1} - val loss increased for {consec_increases} consecutive epochs!\")\n",
    "            break\n",
    "        \n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = network(1, 6, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "train_loss, valid_loss = trainer(model, criterion, optimizer, trainloader, validloader, epochs=201, patience=3)\n",
    "\n",
    "plot_loss(train_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- There are more advanced implementations of early stopping out there, but the idea is the same"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regularization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that regularization is a technique to help avoid overfitting\n",
    "\n",
    "- There are many regularization techniques available in neural networks\n",
    "\n",
    "- I'll discuss the two main ones here:\n",
    "\n",
    "    1. Drop out\n",
    "    \n",
    "    2. L2 regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Drop Out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop out is a common regularization technique and is very simple\n",
    "\n",
    "- Basically, each iteration, we randomly choose some neurons in a layer and don't update their weights (to do this we set the output of the nodes to 0)\n",
    "- A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer = torch.nn.Dropout(p=0.5)  # 50% probability that a node will be set to 0 (\"dropped out\")\n",
    "inputs = torch.randn(5, 3)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above, note how about 50% of nodes have been given a value of 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### L2 Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that in L2 we had this penalty to the loss: $\\frac{\\lambda}{2}||w||^2$\n",
    "\n",
    "- $\\lambda$ is the regularization parameter\n",
    "\n",
    "- L2 regularization is called \"weight-decay\" in PyTorch, the value of which specifies $\\lambda$\n",
    "\n",
    "- It's an argument in most optimizers which you can specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lecture Exercise: Putting it all Together with Bitmojis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- Let's put everything we learned in this lecture together to predict some bitmojis\n",
    "\n",
    "- I have a folder of images with the following structure:\n",
    "\n",
    "```\n",
    "data\n",
    " bitmoji\n",
    "     train\n",
    "        eva\n",
    "        not_eva\n",
    "     valid\n",
    "         eva\n",
    "         not_eva\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"data/eva_bitmoji_rgb/train/\"\n",
    "VALID_DIR = \"data/eva_bitmoji_rgb/valid/\"\n",
    "\n",
    "IMAGE_SIZE = (128, 128)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB')),  # Convert image to RGB    \n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    # transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Training data\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.class_to_idx)  # See which labels are assigned to each class\n",
    "sample_batch = next(iter(train_loader))\n",
    "plot_bitmojis(sample_batch)\n",
    "print(sample_batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now the network\n",
    "\n",
    "- I'm going to make a function `linear_block()` to help create my network and keep things DRY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_block(input_size, output_size):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, output_size),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(0.1)\n",
    "    )\n",
    "\n",
    "class BitmojiClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.main = nn.Sequential(\n",
    "            linear_block(input_size, 128),\n",
    "            linear_block(128, 64),\n",
    "            linear_block(64, 32),\n",
    "            linear_block(32, 16),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Now the training function\n",
    "\n",
    "- This is getting long but it's just all the bits we've seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, patience=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    consec_increases = 0\n",
    "    \n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        \n",
    "        train_batch_loss = 0\n",
    "        train_batch_acc = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "\n",
    "            optimizer.zero_grad()                            # Zero all the gradients w.r.t. parameters\n",
    "\n",
    "            #y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n",
    "            y_hat = model(X).squeeze()  # Forward pass to get output\n",
    "            y_hat_labels = torch.sigmoid(y_hat) > 0.5        # convert probabilities to False (0) and True (1)\n",
    "            loss = criterion(y_hat, y.type(torch.float32))   # Calculate loss based on output            \n",
    "            loss.backward()                                  # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()                                 # Update parameters\n",
    "\n",
    "            train_batch_loss += loss.item()                  # Add loss for this batch to running total\n",
    "            train_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch\n",
    "            \n",
    "        train_loss.append(train_batch_loss / len(trainloader))     # loss = total loss in epoch / number of batches = loss per batch\n",
    "        train_accuracy.append(train_batch_acc / len(trainloader))  # accuracy\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # this turns off those random dropout layers, we don't want them for validation!\n",
    "        \n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "\n",
    "            for X, y in validloader:                \n",
    "                #y_hat = model(X.view(X.shape[0], -1)).flatten()  # Forward pass to get output\n",
    "                y_hat = model(X).squeeze()\n",
    "                y_hat_labels = torch.sigmoid(y_hat) > 0.5        # convert probabilities to False (0) and True (1)\n",
    "                loss = criterion(y_hat, y.type(torch.float32))   # Calculate loss based on output\n",
    "\n",
    "                valid_batch_loss += loss.item()                  # Add loss for this batch to running total\n",
    "                valid_batch_acc += (y_hat_labels == y).type(torch.float32).mean().item()   # Average accuracy for this batch\n",
    "                \n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        \n",
    "        model.train()  # turn back on the dropout layers for the next training loop\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1:3}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Train Accuracy: {train_accuracy[-1]:.2f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 0 and valid_loss[-1] > valid_loss[-2]:\n",
    "            consec_increases += 1\n",
    "        else:\n",
    "            consec_increases = 0\n",
    "        if consec_increases == patience:\n",
    "            print(f\"Stopped early at epoch {epoch + 1:3}: val loss increased for {consec_increases} consecutive epochs!\")\n",
    "            break\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"train_accuracy\": train_accuracy,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = BitmojiClassifier(3 * IMAGE_SIZE[0] * IMAGE_SIZE[1])\n",
    "# model = BitmojiClassifier(IMAGE_SIZE[0] * IMAGE_SIZE[1])\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "results = trainer(model, criterion, optimizer, train_loader, valid_loader, epochs=20, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(results[\"train_loss\"], results[\"valid_loss\"], results[\"train_accuracy\"], results[\"valid_accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- I couldn't get very good accuracy with this model and there's a reason for that: we're not considering the structure in our image\n",
    "\n",
    "- We're flattening our images down into independent pixels, but the relationship between pixels is probably important\n",
    "\n",
    "- We'll exploit that next lecture when we get to CNNs\n",
    "\n",
    "- For now, let's see if our NN model can correctly classify Eva's bitmoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread(\"img/test-examples/eva-well-done.png\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_unseen(image_path): \n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Apply the transformations\n",
    "    transformed_image = data_transforms(image)\n",
    "    \n",
    "    # Add a batch dimension (as model expects batches)\n",
    "    transformed_image = transformed_image\n",
    "    \n",
    "    # Flatten the image\n",
    "    flattened_image = transformed_image.view(1, -1)\n",
    "    print(flattened_image.shape)\n",
    "    # Make sure the flattened image has the correct shape\n",
    "    assert flattened_image.shape == (1, 3* 128 * 128), \"Image tensor shape is not matching model's input shape\"\n",
    "    \n",
    "    # Make a prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(flattened_image)\n",
    "        prediction = torch.sigmoid(prediction)\n",
    "        label = int(prediction > 0.5)\n",
    "        print(label)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(f\"Prediction: {['Eva', 'Not Eva'][label]}\", pad=10, fontsize=20)\n",
    "    # Adjust the following line if working with RGB images\n",
    "    plt.imshow(transformed_image.squeeze(0).permute(1, 2, 0))  # permute dimensions for RGB\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "import glob\n",
    "\n",
    "for unseen_ex in glob.glob(\"img/test-examples/*\"):\n",
    "    predict_unseen(unseen_ex)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that any continuous function can be approximated arbitrarily well by a neural network with at least 1 hidden layer with a finite number of weights.\n",
    "\n",
    "- In other words, neural networks are universal function approximators.\n",
    "\n",
    "- So, how come we weren't able to train a near-perfect image classifier in the previous section?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Highlights\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. Neural network training in a nutshell: forward pass -> calculate loss -> backpropagate -> optimizer step -> repeat. In PyTorch we can customize this process with extras like calculating the loss or accuracy.\n",
    "\n",
    "2. PyTorch takes care of calculating gradients for us with **autograd**.\n",
    "\n",
    "3. Common ways to avoid overfitting: early stopping, drop out, L2 penalty."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dsci572",
   "language": "python",
   "name": "dsci572"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f821000d0c0da66e5bcde88c37d59c8e0de03b40667fb62009a8148ca49465a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
