{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/572_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1: Floating Point Numbers\n",
    "\n",
    "**Tomas Beuzen, January 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/floating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Outline\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lecture-Learning-Objectives\" data-toc-modified-id=\"Lecture-Learning-Objectives-2\">Lecture Learning Objectives</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\">Imports</a></span></li><li><span><a href=\"#1.-Floating-Point-Errors\" data-toc-modified-id=\"1.-Floating-Point-Errors-4\">1. Floating Point Errors</a></span></li><li><span><a href=\"#2.-Binary-Numbers-and-Integers\" data-toc-modified-id=\"2.-Binary-Numbers-and-Integers-5\">2. Binary Numbers and Integers</a></span></li><li><span><a href=\"#3.-Fractional-Numbers-in-Binary\" data-toc-modified-id=\"3.-Fractional-Numbers-in-Binary-6\">3. Fractional Numbers in Binary</a></span></li><li><span><a href=\"#4.-Fixed-Point-Numbers\" data-toc-modified-id=\"4.-Fixed-Point-Numbers-7\">4. Fixed Point Numbers</a></span></li><li><span><a href=\"#5.-Floating-Point-Numbers\" data-toc-modified-id=\"5.-Floating-Point-Numbers-8\">5. Floating Point Numbers</a></span></li><li><span><a href=\"#6.-Spacing-and-Rounding-Errors\" data-toc-modified-id=\"6.-Spacing-and-Rounding-Errors-9\">6. Spacing and Rounding Errors</a></span></li><li><span><a href=\"#7.-Lecture-Exercise:-Fun-with-Floating-Points\" data-toc-modified-id=\"7.-Lecture-Exercise:-Fun-with-Floating-Points-10\">7. Lecture Exercise: Fun with Floating Points</a></span></li><li><span><a href=\"#8.-The-Lecture-in-Three-Conjectures\" data-toc-modified-id=\"8.-The-Lecture-in-Three-Conjectures-11\">8. The Lecture in Three Conjectures</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare and contrast the representations of integers and floating point numbers\n",
    "- Explain how double-precision floating point numbers are represented by 64 bits\n",
    "- Identify common computational issues caused by floating point numbers, e.g., rounding, overflow, etc.\n",
    "- Calculate the \"spacing\" of a 64 bit floating point number in Python\n",
    "- Write code defensively against numerical errors\n",
    "- Use `numpy.iinfo()`/`numpy.finfo()` to work out the possible numerical range of an integer or float dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.floating_point import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Floating Point Errors\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unfortunately, computers can not always represent numbers as accurately as some of us might think\n",
    "- The problem boils down to this: we have a finite amount of bits in our computer to store an infinite amount of numbers\n",
    "- This inevitably leads to precision errors as we'll explore in this lecture\n",
    "- Some motivating examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.2 == 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e16 + 1 == 1e16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this lecture, we are going to take a journey to discover why this happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Numbers and Integers\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are used to using a base-10 number system (i.e., \"decimal\" or \"denary\")\n",
    "- For example, we can read the number 104 as:\n",
    "    - 4 x 1   ($10^0$)\n",
    "    - 0 x 10  ($10^1$)\n",
    "    - 1 x 100 ($10^2$)\n",
    "- Or in a tabular format:\n",
    "\n",
    "|Unit|$10^2$ |$10^1$ |$10^0$ |\n",
    "|:---|:----|:----|:----|\n",
    "|Value|100   |10   |   1|\n",
    "|Number of units|1   |0   |   4|\n",
    "|**Total**|**100**   |**0**   |   **4**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many of you will be aware that computers use a base-2 system (i.e., \"binary\") to store data\n",
    "- Binary is based on powers of 2 rather than powers of 10 and we only have two numbers (0 and 1) to work with (think of them as \"off\" and \"on\" switches respectively)\n",
    "- If you're interested in learning more about how computers work under the hood, check out [this excellent Youtube series on computer science by CrashCourse](https://www.youtube.com/playlist?list=PL8dPuuaLjXtNlUrzyH5r6jN9ulIgZBpdo&app=desktop)\n",
    "- In binary, 104 looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1101000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{104:b}\"  # I'm using the python f-string format code \"b\" to display my integer in binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So this is:\n",
    "    \n",
    "|Unit|$2^6$ |$2^5$ |$2^4$ |$2^3$ |$2^2$ |$2^1$ |$2^0$ |\n",
    "|:---|:----|:----|:----|:---|:---|:---|:---|\n",
    "|Value|64   |32   |   16|   8|   4|   2| 1|\n",
    "|Number of units|1   |1   |   0|   1|   0|   0| 0|\n",
    "|**Total**|**64**   |**32**   | **0** | **8**| **0**| **0**| **0**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We call these single binary 0/1 values, **bits**\n",
    "- So we needed 7 bits to represent the number 104\n",
    "- We can confirm using the `.bit_length()` integer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 104\n",
    "x.bit_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are the first 10 positive integers in binary (using 4 bits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECIMAL | BINARY\n",
      "================\n",
      "   00   |  0000\n",
      "   01   |  0001\n",
      "   02   |  0010\n",
      "   03   |  0011\n",
      "   04   |  0100\n",
      "   05   |  0101\n",
      "   06   |  0110\n",
      "   07   |  0111\n",
      "   08   |  1000\n",
      "   09   |  1001\n",
      "   10   |  1010\n"
     ]
    }
   ],
   "source": [
    "print(\"DECIMAL | BINARY\")\n",
    "print(\"================\")\n",
    "for _ in range(11):\n",
    "    print(f\"   {_:02}   |  {_:04b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- And a useful GIF counting up in binary to drive the point home:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/binary-denary.gif)\n",
    "\n",
    "Source: [futurelearn.com](https://www.futurelearn.com/courses/how-computers-work/0/steps/49261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Any integer can be represented in binary (if we have enough bits)\n",
    "- The range of *unsigned integers* we can represent with `N` bits is: 0 to $2^{N}-1$\n",
    "- We often reserve one bit to specify the sign (- or +) of a number, so the range of *signed integers* you can represent with `N` bits is: $-2^{N-1}$ to $2^{N-1}-1$\n",
    "- For example, with 8 bits we can represent the unsigned integers 0 to 255, *or* the signed integers -128 to 127:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsigned min: 0\n",
      "unsigned max: 255\n",
      "\n",
      "signed min: -128\n",
      "signed max: 127\n"
     ]
    }
   ],
   "source": [
    "bits = 8\n",
    "print(f\"unsigned min: 0\")\n",
    "print(f\"unsigned max: {2 ** bits - 1}\\n\")\n",
    "print(f\"signed min: {-2 ** (bits - 1)}\")\n",
    "print(f\"signed max: {2 ** (bits - 1) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can confirm all this with the numpy function `np.iinfo()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-128, max=127, dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(\"int8\")  # use np.iinfo() to check out the limits of a dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=0, max=255, dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(\"uint8\")  # \"uint8\" is an unsigned 8-bit integer, so ranging from 0 to 2^7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending on the language/library you're using, you may get an error or some other funky behaviour if you try to store an integer smaller/larger than the allowable\n",
    "- In the case of `numpy`, it will loop around and continue counting from the upper/lower limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-87,  31], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([-87, 31], \"int8\")  # this is fine as all numbers with the allowable range of int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 127, -128], dtype=int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([-129, 128], \"int8\")  # these numbers are outside the allowable range!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To store the above, we need more bits (memory)\n",
    "- For example, 16 bits will be more than enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-129,  128], dtype=int16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([-129, 128], \"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You often see bits in multiples of 8 (e.g., 8, 16, 32, 64, etc)\n",
    "- 8 bits is traditionally called 1 \"byte\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the Python default `int` has 64-bits, so a range of $-2^{63}$ to $2^{63} - 1$. But Python is special in that it can dynamically allocate more memory to hold your integer if needed, but that's not really important for us right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18446744073709551616"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 2 ** 64\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.bit_length()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fractional Numbers in Binary\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this point you might be thinking, \"Well that's all well and good for integers, but what about fractional numbers like 14.75 Tom?\" - Good question!\n",
    "- Let's interpret the number 14.75 in our familiar decimal format\n",
    "    \n",
    "|Unit|$10^1$ |$10^0$ |$10^{-1}$ |$10^{-2}$ |\n",
    "|:---|:----|:----|:----|:---|\n",
    "|Value|10   |1   |   0.1|   0.01|\n",
    "|Number of units|1   |4   |   7|   5|\n",
    "|**Total**|**10**   |**4**  |   **0.7**|  **0.05**|\n",
    "\n",
    "- That seems pretty natural, and in binary it's much the same!\n",
    "- Anything to the right of the decimal point (now called a \"binary point\") is treated with a **negative exponent**\n",
    "- So in binary, 14.75 would look like this: 1110.11\n",
    "- Let's break that down:\n",
    "\n",
    "|Unit|$2^3$ |$2^2$ |$2^1$ |$2^0$ |$2^{-1}$ |$2^{-2}$ |\n",
    "|:---|:----|:----|:----|:---|:---|:---|\n",
    "|Value|8   |4   |   2|   1|   0.5|   0.25|\n",
    "|Number of units|1   |1   |   1|   0|   1|   1|\n",
    "|**Total**|**8**   |**4**   | **2** | **0**| **0.5**| **0.25**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fixed Point Numbers\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we learned earlier, we typically have a fixed number of bits to work with when storing a number, e.g., 8, 16, 32, 64 bits etc.\n",
    "- How do we decide where to put the binary point to allow for fractional numbers?\n",
    "- With 8 bits, we could put 4 bits on the left and 4 on the right:\n",
    "\n",
    "|Unit|$2^3$ |$2^2$ |$2^1$ |$2^0$ |$2^{-1}$ |$2^{-2}$|$2^{-3}$|$2^{-4}$|\n",
    "|---|----|----|---|---|---|---|---|---|\n",
    "|Value|8   |   4|   2|   1|   0.5|   0.25|   0.125|   0.0625|\n",
    "\n",
    "- In this case, the largest and smallest (closest to 0) numbers we could represent in the unsigned case are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.9375"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.0 ** np.arange(-4, 4, 1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But what if we wanted to represent numbers larger than this?\n",
    "- We could shift the binary point right, so we have 6 on the left and 2 on the right, then our range would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2.0 ** np.arange(-2, 6, 1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get this trade-off between being able to represent large numbers and small numbers\n",
    "- What if we want to represent both very large and very small numbers? Read on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Floating Point Numbers\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this point, you may have read \"floating point\" and had a little \"ah-ha!\" moment because you see where we're going\n",
    "- Rather than having a fixed location for our binary point, we could let it \"float\" around depending on what number we want to store\n",
    "- Recall \"scientific notation\", which for the number 1234 looks like $1.234 \\times 10^3$, or for computers we usually use `e` for shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.234e+03'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{1234:.3e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The \"exponent\" controls the location of the decimal point\n",
    "- Consider the following numbers:\n",
    "    - $1.234 \\times 10^0 = 1.234$\n",
    "    - $1.234 \\times 10^1 = 12.34$\n",
    "    - $1.234 \\times 10^2 = 123.4$\n",
    "    - $1.234 \\times 10^3 = 1234.$\n",
    "- See how by changing the value of the exponent we can control the location of the floating point and represent a range of values?\n",
    "- We'll be using the exact same logic with the binary system to come up with our \"floating point numbers\"\n",
    "- This is floating point format:\n",
    "\n",
    "$$1.M \\times 2^E$$\n",
    "\n",
    "- $M$ = \"mantissa\"\n",
    "- $E$ = \"exponent\"\n",
    "- Note that the $.$ is a \"binary point\", digits on the left are 2's with +ve exponents, digits on the right are 2's with negative exponents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the number 10\n",
    "- $10 = 1.25 \\times 8 = 1.25 \\times 2^3$\n",
    "- So $M=.25$ and $E=3$\n",
    "- But we want binary, not decimal, so $M=01$ and $E=11$\n",
    "- Therefore, 10 in floating point is: $1.01 \\times 2^{11}$\n",
    "- This is where the magic happens, just as the exponent of 10 in scientific notation defines the location of the decimal point, so too does the exponent of a floating point number define the location of the binary point\n",
    "- For $1.01 \\times 2^{11}$, the exponent is 11 in binary = 3 in decimal, so move the binary point three places to the right: $1.01 \\times 2^{11} = 1010.$\n",
    "- What is 1010 in binary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(\"1010\", base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's 10 of course!\n",
    "- How cool is that - we now have this \"floating point\" data type that uses an exponent to help us represent both small and large fractional numbers (unlike fixed point where we would have to choose one or the other!)\n",
    "- I wrote a function `binary()` to display any number in floating point format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decimal: 1.25 x 2^3\n",
      "  Binary: 1.01 x 2^11\n",
      "\n",
      "    Sign: 0 (+)\n",
      "Mantissa: 01 (0.25)\n",
      "Exponent: 11 (3)\n"
     ]
    }
   ],
   "source": [
    "binary(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try the speed of light: $2.998 \\times 10 ^ 8 m/s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decimal: 1.11684203147888184 x 2^28\n",
      "  Binary: 1.0001110111101001010111 x 2^11100\n",
      "\n",
      "    Sign: 0 (+)\n",
      "Mantissa: 0001110111101001010111 (0.11684203147888184)\n",
      "Exponent: 11100 (28)\n"
     ]
    }
   ],
   "source": [
    "binary(2.998e8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A mole: $6.02214076 \\times 10^{23} particles$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decimal: 1.9925592330949422 x 2^78\n",
      "  Binary: 1.1111111000011000010111001010010101111100010100010111 x 2^1001110\n",
      "\n",
      "    Sign: 0 (+)\n",
      "Mantissa: 1111111000011000010111001010010101111100010100010111 (0.9925592330949422)\n",
      "Exponent: 1001110 (78)\n"
     ]
    }
   ],
   "source": [
    "binary(6.02214076e23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Planck's constant: $6.62607004 \\times 10^{-34} m^2 kg / s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decimal: 1.720226132656187 x 2^-111\n",
      "  Binary: 1.1011100001100000101111010110010101111011100111100001 x 2^-1101111\n",
      "\n",
      "    Sign: 0 (+)\n",
      "Mantissa: 1011100001100000101111010110010101111011100111100001 (0.720226132656187)\n",
      "Exponent: -1101111 (-111)\n"
     ]
    }
   ],
   "source": [
    "binary(6.62607004e-34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You get the point, we can represent a wide range of numbers with this format! (we'll work out the exact range later on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Floating Point Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One question you might have about the above: how many bits should I use for the mantissa and how many for the exponent?\n",
    "- Well that's already been decided for you in: IEEE Standard for Floating-Point Arithmetic ([IEEE 754](https://en.wikipedia.org/wiki/IEEE_754)) which is what most computers/software use\n",
    "- You'll mostly be using the data types `float64` and `float32`\n",
    "- Float 64 (also called \"double precision\") \n",
    "  - 53 bits for the mantissa\n",
    "  - 11 bits for the exponent\n",
    "- Float 32 (also called \"single precision\") \n",
    "  - 24 bits for the mantissa\n",
    "  - 8 bits for the exponent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spacing and Rounding Errors\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Ingredient 1: Rounding Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many fractional numbers can't be represented exactly using binary\n",
    "- **Instead, your computer will round numbers to the closest representable number and we get rounding errors**\n",
    "- For example 0.1 can't be exactly represented in binary (feel free to try and make 0.1 using floating point format).\n",
    "- Python usually hides this fact for us out of convenience, but here is \"*the real 0.1*\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.100000000000000005551115123125782702118158340454101562500000'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{0.1:.60f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So 0.1 is actually represented as a number slightly bigger than 0.1!\n",
    "- I wrote a function to work out if a number is stored exactly or inexactly, and to show the rounding error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: 0.1\n",
      "Which is inexactly stored as: 0.1000000000000000055511151231257827021181583404541015625\n"
     ]
    }
   ],
   "source": [
    "float_rep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You entered: 0.25\n",
      "Which is exactly stored as: 0.25\n"
     ]
    }
   ],
   "source": [
    "float_rep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read more about float representation in the Python docs [here](https://docs.python.org/3/tutorial/floatingpoint.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Ingredient 2: Spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So how bad are these rounding errors? Let's find out...\n",
    "- We can quantify the \"spacing\" between representable numbers\n",
    "- Imagine we're in the decimal system again. For a number with a fixed amount of significant digits, the spacing between that number and the next biggest number with the same format can be determined as the smallest significant digit multipled by the exponent:\n",
    "\n",
    "|Number| Next Largest| Spacing|\n",
    "|---|---|---|\n",
    "|8.982e0| 8.983e0 | 0.001e0 = 0.001|\n",
    "|0.001e1| 0.002e1 | 0.001e1 = 0.01|\n",
    "|3.423e2| 3.424e2 | 0.001e2 = 0.1|\n",
    "\n",
    "- Same goes for our binary floating point numbers!\n",
    "- The spacing can be determined as the smallest part of the mantissa multiplied by a number's exponent\n",
    "- What is the smallest part of the mantissa?\n",
    "- For `float64`, we have 53 bits for the mantissa, 1 bit is reserved for the sign, so we have 52 left, therefore the smallest significant digit is $2^{-52}$\n",
    "- To be super clear, here is the floating point format written as powers of 2 for all 52 mantissa bits and 11 exponent bits:\n",
    "\n",
    "$$2^0{}.{}2^{-1}2^{-2}2^{-3}...{}2^{-50}2^{-51}\\boldsymbol{2^{-52}} \\times 2 ^ {2^{0}2^{1}2^{2}...{}2^{9}2^{10}2^{11}}$$\n",
    "\n",
    "- See how $2^{-52}$ is the smallest value we can change?\n",
    "- Consider the number 1 in binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decimal: 1.0 x 2^0\n",
      "  Binary: 1.0 x 2^0\n",
      "\n",
      "    Sign: 0 (+)\n",
      "Mantissa: 0 (0.0)\n",
      "Exponent: 0 (0)\n"
     ]
    }
   ],
   "source": [
    "binary(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see, the number 1 has has an exponent of 0\n",
    "- Remember the smallest significant digit scaled by our exponent gives us the spacing between this number, and the next number we can represent with our format\n",
    "- So for the number 1, the spacing is $2^{-52} * 2^0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2 ** -52) * (2 ** 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpy has a helpful function we can use to calculate spacing\n",
    "- `np.nextafter(x1, x2)` returns the next representable floating-point value after `x1` towards `x2`\n",
    "- The next representable number after 1 should be 1 plus the spacing above, let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nextafter(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacing = np.nextafter(1, 2) - 1\n",
    "spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacing == (2 ** -52) * (2 ** 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you do a calculation that puts you somewhere between the **space** of two numbers, the computer will automatically round to the nearest one\n",
    "- I like to think of this as a mountain range in the computer\n",
    "- In the valleys are representable numbers, if a calculation puts us on a mountain side, we'll roll down the mountain to the closest valley\n",
    "- For example, spacing for the number 1 is $2^{-52}$, so if we don't add at least half that, we won't reach the next representable number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 0.4 * spacing == 1  # add less than half the spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/fall_left.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 0.6 * spacing == 1 # add a more than half the spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/fall_right.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We're working with pretty small numbers here so you might not be too shocked\n",
    "- But remember, the bigger our exponent, the bigger our rounding errors will be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_number = 1e25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Decimal: 1.03397576569128469 x 2^83\n",
      "  Binary: 1.0000100010110010101000101100001010000000001010010001 x 2^1010011\n",
      "\n",
      "    Sign: 0 (+)\n",
      "Mantissa: 0000100010110010101000101100001010000000001010010001 (0.03397576569128469)\n",
      "Exponent: 1010011 (83)\n"
     ]
    }
   ],
   "source": [
    "binary(large_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The exponent is 83 so the spacing should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14748e+09\n"
     ]
    }
   ],
   "source": [
    "spacing = (2 ** -52) * (2 ** 83)\n",
    "print(f\"{spacing:.5e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 billion is less than half that spacing, so if we add it to `large_number`, we won't cross the mountain peak, and we'll slide back down into the same valley:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_billion = 1e9\n",
    "1e25 + one_billion == 1e25  # adding a billion didn't change our number!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 billion is more than half that spacing, so if we add it to `large_number`, we'll slide into the next valley (representable number):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_billion = 2e9\n",
    "1e25 + two_billion == 1e25  # adding two billion (more than half the spacing) did change our number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another implication of having a finite amount of floating numbers to represent an infinite number system is that two numbers may share the same representation in memory\n",
    "- For example, these are all `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.99999999999999999 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.00000000000000004 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.00000000000000009 == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is pretty intuitive is you think about the \"Floating Point Mountain Range\"\n",
    "- No matter what number you are, if you don't pass over the peak to your left or right, you'll roll back down to the valley!\n",
    "- And also remember, the bigger the number, the bigger the spacing/rounding errors!\n",
    "\n",
    "![](img/mountains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lecture Exercise: Fun with Floating Points\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Floating Point Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that most compute environments you'll encounter will be using IEEE double precision (`float64`), so 53 bits for the mantissa, 11 for the exponent.\n",
    "- So let's calculate the biggest possible floating point number based on this scheme.\n",
    "- Recall that if we have 11 bits for the exponent then the **signed range** of possible values is: $-2^{N-1}$ to $2^{N-1}-1$, and the largest value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits = 11\n",
    "max_exponent = 2 ** (bits - 1) - 1\n",
    "max_exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For `float64` the maximum mantissa can be calculated as the sum of all 52 bits from ($2^{-1}$ to $2^{-52}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_mantissa = (2.0 ** np.arange(-1, -53, -1)).sum()\n",
    "max_mantissa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Therefore the maximum float we can represent is (remember our format is $1.M \\times 2 ^ E$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + max_mantissa) * 2 ** max_exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's confirm using the `numpy` function `np.finfo()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e+308"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.float64).max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks good to me!\n",
    "- What happens if we try to make a number bigger than this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-042b824954e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m309.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "10 ** 309.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or in `numpy`, it throws a warning and defaults to `inf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mds572/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in power\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(10, 309.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's do the minimum value\n",
    "- The minimum value will be the minimum mantissa multiplied by the minimum exponent\n",
    "- The minimum mantissa is 0 (all bits sets to 0)\n",
    "- You might think the minimum exponent is $-2^{10}=-1024$ just as we've discussed through this lecture, but there are some bit patterns reserved for \"special numbers\" (like 0) which effectively reduce the minimum exponent to $-1022$ (this is not important to know but you can read more [here](https://people.orie.cornell.edu/snp32/orie_6125/ieee754/ieee754.html))\n",
    "- So the minimum value is actually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 * 2 ** -1022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You don't need to worry about this derivation too much, after all `numpy` can help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1022"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.float64).minexp  # minimum exponent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(np.float64).tiny  # smallest possible positive value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) 7.1.1 Subnormal Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What happens if we try to store a number smaller than the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.562684646268003e-309"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** -1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Um, it worked... 🤔\n",
    "- Remember how I discussed \"special cases\" earlier? One of these is \"[subnormal numbers](https://en.wikipedia.org/wiki/Denormal_number)\"\n",
    "- This is well beyond the scope of the course, but briefly, subnormal numbers help us fill the gap between 0 and $2^{-1022}$\n",
    "- If the exponent is set to it's minimum value, then we can play with our bits (that's not meant to sound dirty), and instead of the mantissa starting with a 1, we can start it with a 0\n",
    "- This gives our mantissa a minimum value of $2^{-52}$ (rather than 1, which is what we had before)\n",
    "- So the smallest subnormal number we can have is $2^{-52} \\times 2 ^ {-1022} = 2 ^ {-1074}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-324"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** -1074"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Anything less than this is rounded to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** -1075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can read more [in this article](https://people.orie.cornell.edu/snp32/orie_6125/ieee754/ieee754.html#subnormal-numbers) and this [stackoverflow post](https://stackoverflow.com/a/14002396/14179508), but as I said, this is beyond the scope of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Order of Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e16 + 1 + 1 == 1 + 1 + 1e16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What's up with that?\n",
    "- Let's break it down...\n",
    "- First the spacing of `1e16` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_spacing(1e16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e16 + 1 == 1e16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the above should make sense, we aren't adding more than half the spacing, so we remain at `1e16`\n",
    "- But what about this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e16 + (1 + 1) == 1e16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This time we added `(1 + 1)`, which is `2`, and we got a `False` as expected\n",
    "- But now let's remove the parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e16 + 1 + 1 == 1e16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back to `True`, what's going on...\n",
    "- Well we have an \"order of operations\" here, the `1e16 + 1` happens first, which is just `1e16`, and then we try add `1` again but the same thing happens\n",
    "- What do you think will happen with this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1 + 1e16 == 1e16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This time, the `1 + 1` happened first, which is `2`, and then we add `1e16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Writing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Later in this course we'll create a Logistic Regression model from scratch and you'll have to calculate something that looks like:\n",
    "\n",
    "$$\\log(1+\\exp(z))$$\n",
    "\n",
    "- Looks pretty harmless right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_1_plus_exp(z):\n",
    "    return np.log(1 + np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But what happens if `z` is large? Say, 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mds572/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_1_plus_exp(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get an overflow error when `z` in `exp(z)` is large!\n",
    "- In fact, we can find out what the limit of `z` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709.782712893384"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_float = np.finfo(np.float64).max\n",
    "np.log(max_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we pass anything bigger than that into `np.exp()` we'll get an overflow error\n",
    "- However, you might know that for $z >> 1$:\n",
    "\n",
    "$$\\log(1+\\exp(z))\\approx \\log(\\exp(z)) = z$$\n",
    "\n",
    "- e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 100\n",
    "np.log(1 + np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use this to account for the overflow error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize  # decorator to vectorize the function\n",
    "def log_1_plus_exp_safe(z):\n",
    "    if z > 100:\n",
    "        print(f\"Avoiding overflow error with approximation of {z:.0f}!\")        \n",
    "        return z\n",
    "    else:\n",
    "        return np.log(1 + np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avoiding overflow error with approximation of 200!\n",
      "Avoiding overflow error with approximation of 500!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1.31326169,  10.0000454 , 200.        , 500.        ])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_1_plus_exp_safe([1, 10, 200, 500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cool! We combined math + CS + our brains to write better code! You'll explore this more in Lab 1\n",
    "- But note that this is part of the reason we use libraries like `numpy`, `scipy`, `scikit-learn`, etc. They've taken care of all of this for you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Precision vs Memory vs Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So double precision floats are pretty standard and require 64 bits to store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float64().nbytes  # number of bytes consumed by a float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float64().nbytes * 8  # recall 1 byte = 8 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When dealing with huge datasets or models with millions or billions of parameters it may be desirable to lower the precision of our numbers to use less memory and/or gain speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array size: (1000, 1000)\n",
      "array type: float64\n",
      "mem. usage: 64.0 MB\n"
     ]
    }
   ],
   "source": [
    "x64 = np.random.randn(1000, 1000)\n",
    "print(f\"array size: {x64.shape}\")\n",
    "print(f\"array type: {x64.dtype}\")\n",
    "print(f\"mem. usage: {x64.nbytes * 8 / (1000 * 1000)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array type: float32\n",
      "mem. usage: 32.0 MB\n"
     ]
    }
   ],
   "source": [
    "x32 = x64.astype('float32')\n",
    "print(f\"array type: {x32.dtype}\")\n",
    "print(f\"mem. usage: {x32.nbytes * 8 / (1000 * 1000)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below I'm squaring the elements of my two arrays. They have the same number of elements, but different data types - let's observe the difference in the speed of the operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 array is 6.78x faster than float64 array here.\n"
     ]
    }
   ],
   "source": [
    "time64 = %timeit -q -o -r 3 x64 ** 2\n",
    "time32 = %timeit -q -o -r 3 x32 ** 2\n",
    "print(f\"float32 array is {time64.average / time32.average:.2f}x faster than float64 array here.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I'm showing you this as foreboding for later topics dealing with memory and deep neural networks, where we'll use these ideas of precision vs speed to optimize our models!\n",
    "\n",
    "![](img/foreboding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Lecture in Three Conjectures\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Most fractional numbers are not represented exactly by floating point numbers in computers which can lead to rounding errors.\n",
    "2. Most compute environments you'll encounter will use IEEE double precision... but others do exist (especially single precision). Some software will require you to use a particular data type due to computational limitations (for example, PyTorch sometimes forces you to use float32 instead of float64).\n",
    "3. There is a biggest and smallest floating point number (depending on precision), beyond these we get overflow or underflow errors. Use `np.nextafter()` and `numpy.finfo()` to work out float spacing and ranges.\n",
    "\n",
    "![](img/mission.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mds572]",
   "language": "python",
   "name": "conda-env-mds572-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Lecture Outline",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
