{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/572_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Convolutional Neural Networks\n",
    "\n",
    "**Tomas Beuzen, January 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quiz week and we've been hitting it hard in lectures so I've got a nice short lab for you this week. I'll be getting you to review basic CNN concepts and to implement a CNN from scratch in PyTorch - let's do it! Beep boop beep boop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Instructions\" data-toc-modified-id=\"Instructions-2\">Instructions</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\">Imports</a></span></li><li><span><a href=\"#Exercise-1:-Filters-and-Convolutions\" data-toc-modified-id=\"Exercise-1:-Filters-and-Convolutions-4\">Exercise 1: Filters and Convolutions</a></span></li><li><span><a href=\"#Exercise-2:-CNNs-and-Image-Permutations\" data-toc-modified-id=\"Exercise-2:-CNNs-and-Image-Permutations-5\">Exercise 2: CNNs and Image Permutations</a></span></li><li><span><a href=\"#Exercise-3:-Bitmoji-CNN\" data-toc-modified-id=\"Exercise-3:-Bitmoji-CNN-6\">Exercise 3: Bitmoji CNN</a></span></li><li><span><a href=\"#(Optional)-Exercise-4:-Neural-Network-and-Backpropagation-&quot;By-Hand&quot;\" data-toc-modified-id=\"(Optional)-Exercise-4:-Neural-Network-and-Backpropagation-&quot;By-Hand&quot;-7\">(Optional) Exercise 4: Neural Network and Backpropagation \"By Hand\"</a></span></li><li><span><a href=\"#Submit-to-Canvas-and-GitHub\" data-toc-modified-id=\"Submit-to-Canvas-and-GitHub-8\">Submit to Canvas and GitHub</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<hr>\n",
    "\n",
    "rubric={mechanics:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4aecf0223bc592cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Link to your GitHub repository:**\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. To submit this assignment you should:\n",
    "\n",
    "1. Push your assignment to your GitHub repository!\n",
    "2. Provide a link to your repository in the space provided above.\n",
    "2. Upload a HTML render of your assignment to Canvas. The last cell of this notebook will help you do that.\n",
    "3. Be sure to follow the [General Lab Instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/). You can view a description of the different rubrics used for grading in MDS [here](https://github.com/UBC-MDS/public/tree/master/rubric).\n",
    "\n",
    "Here's a break down of the required and optional exercises in this lab:\n",
    "\n",
    "|         | Number of Exercises | Points |\n",
    "|:-------:|:-------------------:|:------:|\n",
    "| Required| 9 | 30 |\n",
    "| Optional| 1  | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'font.size': 16, 'axes.labelweight': 'bold', 'axes.grid': False})\n",
    "from canvasutils.submit import submit, convert_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Filters and Convolutions\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an image of my dog Evie (so cute I know!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.from_numpy(plt.imread(\"img/evie.png\"))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6QQPeGXO2pc"
   },
   "source": [
    "For each of the filters given below, convolve the image of Evie with the given filter/kernel and briefly discuss why the results look the way they do. You'll need to:\n",
    "1. Created a `Conv2D` layer with PyTorch.\n",
    "2. Manually change the kernel weights. Weights in a `Conv2D` layer are 4D tensors (the 4 dimensions are: `[num_images=1, num_channels=1, kernel_rows, kernel_cols]`) and changing the `Conv2D` layer weights (I've given example code defining such a 4D tensor below). Functions that will help you create tensors: `torch.ones()`, `torch.zeros()`, `torch.full()`, etc. (remember, we have much of the same functionality as we did in `NumPy`!).\n",
    "3. Use the provided code to plot the original and convolved images.\n",
    "4. Explain the result in one sentence.\n",
    "\n",
    "I've provided an example below to get you started. You can assume the default `stride` and `padding` in the `Conv2D` layer.\n",
    "\n",
    ">The pedagogical goal here is to help you better understand what filters/kernels actually are and how they help us identify useful structure (like lines, curves, shapes, etc.) in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convolution(image, conv_layer):\n",
    "    \"\"\"\n",
    "    Convolve kernel over image and plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : torch.Tensor\n",
    "        Image to filter with kernel.\n",
    "    conv_layer : function\n",
    "        A PyTorch Conv2D layer to apply to image.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.image\n",
    "    \"\"\"\n",
    "\n",
    "    conv_image = conv_layer(image[None, None, :]).detach().squeeze()\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(8, 4), ncols=2)\n",
    "    ax1.imshow(image, cmap='gray'); ax1.axis('off'); ax1.set_title(\"Original\")\n",
    "    ax2.imshow(conv_image, cmap='gray'); ax2.axis('off'); ax2.set_title(\"Filtered\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "The kernel is a row vector of ten 0.1's, shape (1, 10):\n",
    "\n",
    "$$kernel = \\begin{bmatrix} 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=(1, 10))\n",
    "kernel = torch.full((1, 1, 1, 10), 0.1)\n",
    "conv_layer.weight[:] = kernel\n",
    "plot_convolution(image, conv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJfVXMiCO2pp"
   },
   "source": [
    "**Example answer:**\n",
    "\n",
    "The filter is a horizontal bar of $0.1$s. Therefore I would expect a blurring in the horizontal direction, meaning the _vertical_ edges get blurred (because these are the ones that change rapidly in the horizontal direction). This seems to be happening in the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-z9WROaO2pr"
   },
   "source": [
    "### 1.1\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a column vector of ten 0.1's, shape (10, 1):\n",
    "\n",
    "$$kernel = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nUPLjfCO2pr",
    "outputId": "e9fbcee2-ace6-42aa-fd71-6f6fa162069d"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtPIw0w-O2pw"
   },
   "source": [
    "### 1.2\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a matrix of 0's but with a 1 in the centre, shape (5, 5):\n",
    "\n",
    "$$kernel = \\begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAhTDb23O2py",
    "outputId": "601583c5-ecba-4bf3-fec9-e28d9b70d645"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32tx26E6O2p1"
   },
   "source": [
    "### 1.3\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a matrix of 0.01's, shape (10, 10):\n",
    "\n",
    "$$kernel = \\begin{bmatrix} 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & \\\\ 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lj1wDS7yO2p3",
    "outputId": "cde75fe3-7732-4499-8eeb-228416439f93"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32K6eYDKO2p8"
   },
   "source": [
    "### 1.4\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel is a matrix of -0.125's, shape (3, 3):\n",
    "\n",
    "$$kernel = \\begin{bmatrix} -0.125 & -0.125 & -0.125 \\\\ -0.125 & -0.125 & -0.125 \\\\ -0.125 & -0.125 & -0.125 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkjVbsI7O2p9",
    "outputId": "9fffe31d-7f36-449f-9964-9f1aae4ec50e"
   },
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: CNNs and Image Permutations\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gg208bROO2qN"
   },
   "source": [
    "Below is some code that trains a CNN on the classic [MNIST digits dataset](https://en.wikipedia.org/wiki/MNIST_database). This dataset contains 28 x 28 pixel images of hand written digits. Run through the code, it may take a few minutes to run the code, our training dataset has 60,000 samples and our validation dataset has 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "# Download data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "validset = datasets.MNIST('data/', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Sample plot\n",
    "X, y = next(iter(trainloader))\n",
    "plt.imshow(X[0, 0, :, :], cmap=\"gray\")\n",
    "plt.title(f\"Number: {y[0].item()}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (5, 5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    train_loss, valid_loss, valid_accuracy = [], [], []\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        valid_batch_acc = 0\n",
    "        \n",
    "        # Training\n",
    "        for X, y in trainloader:\n",
    "            optimizer.zero_grad()       # Zero all the gradients w.r.t. parameters\n",
    "            y_hat = model(X)            # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss based on output\n",
    "            loss.backward()             # Calculate gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            train_batch_loss += loss.item()  # Add loss for this batch to running total\n",
    "        train_loss.append(train_batch_loss / len(trainloader))\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood and saves memory and time\n",
    "            for X, y in validloader:\n",
    "                y_hat = model(X)\n",
    "                _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n",
    "                loss = criterion(y_hat, y)\n",
    "                valid_batch_loss += loss.item()\n",
    "                valid_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n",
    "        valid_loss.append(valid_batch_loss / len(validloader))\n",
    "        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}:\",\n",
    "                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n",
    "                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n",
    "    \n",
    "    results = {\"train_loss\": train_loss,\n",
    "               \"valid_loss\": valid_loss,\n",
    "               \"valid_accuracy\": valid_accuracy}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training may take a few minutes, we have a lot of data (60,000 training samples)\n",
    "model = MNIST_classifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "results = trainer(model, criterion, optimizer, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably got pretty good accuracy there! Now, answer the questions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4aYezc9O2qZ"
   },
   "source": [
    "How many parameters does the model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 6 I talked about how, when doing image classification with fully connected neural networks, the order of the pixels in the flattened image we feed into the network doesn't matter. In contrast, CNNs try to use the structure in the data to make predictions. Let's do an experiment and vertically flip all our MNIST training images like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.RandomVerticalFlip(p=1), transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Sample plot\n",
    "X, y = next(iter(trainloader))\n",
    "plt.imshow(X[0, 0, :, :], cmap=\"gray\")\n",
    "plt.title(f\"Number: {y[0].item()}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only flipped the training images, not the validation images. What do you think would happen to the validation scores if we train our network on these new flipped images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Your answer goes here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-train your network using the new `trainloader` of flipped images we defined above to test your answer to 2.2. Are the results what you expected? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Bitmoji CNN\n",
    "<hr>\n",
    "\n",
    "rubric={accuracy:15}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the student repository is a folder (`lectures/data/bitmoji_rgb`) containing **coloured** versions of the \"Tom\" and \"Not Tom\" Bitmoji images we saw in Lecture 5. Make sure you clone that repo and then copy the folder `bitmoji_rgb` to this directory (or feel free to change the path below to point to the copy in your cloned student repo). We have 857 images of both \"`tom`\" and \"`not_tom`\" for training (1714 images total), and 200 images of both \"`tom`\" and \"`not_tom`\" for validation (400 images total). We will resize the images to be 64 x 64 pixels.\n",
    "\n",
    "Your task here is to simply create and train a CNN on this data that classifies a given image as \"`tom`\" or \"`not_tom`\" (i.e., this is binary classification). You can create any architecture you wish, but you must show me you have at least 70% accuracy on the validation data set after training your CNN (even with a very simple CNN I got >80% with just 20 epochs). I used a combination of the following layers but you can use whatever you wish:\n",
    "- `torch.nn.Conv2d()`\n",
    "- `torch.nn.ReLU()`\n",
    "- `torch.nn.MaxPool2d()`\n",
    "- `torch.nn.Dropout()`\n",
    "- `torch.nn.Flatten()`\n",
    "- `torch.nn.Linear()`\n",
    "\n",
    "I have prepared the training and validation loaders for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"bitmoji_rgb/train/\"\n",
    "VALID_DIR = \"bitmoji_rgb/valid/\"\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Load data and create datalaoders\n",
    "data_transforms = transforms.Compose([transforms.Resize(IMAGE_SIZE), transforms.ToTensor()])\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=data_transforms)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Plot samples (don't worry too much about this code)\n",
    "sample_batch = next(iter(trainloader))\n",
    "plt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "plt.imshow(np.transpose(utils.make_grid(sample_batch[0], padding=1, normalize=True),(1,2,0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's some code you can use to plot your model's predictions for a random image from the validation set\n",
    "# Depending on how you set your model up you may have to change some things to make dimensions work\n",
    "img = next(iter(trainloader))[0][0]\n",
    "y_prob = torch.sigmoid(model(img.unsqueeze(0)))\n",
    "y_class = int(y_prob > 0.5)\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"{['Not Tom', 'Tom'][y_class]} (P={y_prob.item():.2f})\", pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Exercise 4: Neural Network and Backpropagation \"By Hand\"\n",
    "<hr>\n",
    "\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch using only `NumPy` and `SciPy`, implement a one-hidden-layer neural network for regression using ReLUs. You can ignore the fact that the ReLU function is not differentiable when its input equals zero (it's very unlikely that we'll ever get a value of exactly 0 in our network). You're pretty much on your own for this question - it's a real test of how good your coding skills are and your knowledge of how neural networks actually work. You'll need to code up the gradients and implement backpropagation yourself. **This is a lot of work for one mark, use your time wisely**. \n",
    "\n",
    "Feel free to use any dataset you like to test your implementation. You can borrow the \"non-linear regression\" dataset I used in [Lecture 4](https://pages.github.ubc.ca/MDS-2020-21/DSCI_572_sup-learn-2_students/lectures/lecture4_pytorch-neural-networks-pt1.html#non-linear-regression-with-a-neural-network) if you like:\n",
    "\n",
    "```python\n",
    "np.random.seed(2020)\n",
    "X = np.sort(np.random.randn(500))\n",
    "y = X ** 2 + 15 * np.sin(X) ** 3\n",
    "```\n",
    "\n",
    "> Note: there are probably a lot of resources out there where people give their \"raw\" neural network implementations. If you're going to do this and you consult any sources, make sure you cite them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7d15fa1e22d412c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Submit to Canvas and GitHub\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are ready to submit your assignment do the following:\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Run All Cells...`\n",
    "2. Save your notebook.\n",
    "3. Convert your notebook to `.html` format using the `convert_notebook()` function below or by `File -> Export Notebook As... -> Export Notebook to HTML`\n",
    "4. Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    "5. Finally, push all your work to GitHub (including the rendered html file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd03c1d25ce1d32e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# convert_notebook(\"lab3.ipynb\", \"html\")  # save your notebook, then uncomment and run when you want to convert to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit(course_code=59090)  # uncomment and run when ready to submit to Canvas"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mds572]",
   "language": "python",
   "name": "conda-env-mds572-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
